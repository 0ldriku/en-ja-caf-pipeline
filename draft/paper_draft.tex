\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{tabularx}

\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}

\title{Validating a Cross-Lingual Pipeline for Automatic Clause-Based Fluency Annotation:\\
Evidence from English and Japanese L2 Speech}
\author{Author names and affiliations to be inserted}
\date{Draft dated February 26, 2026}

\begin{document}
\maketitle

%% ====================================================================
%% ABSTRACT
%% ====================================================================
\begin{abstract}
Automatic annotation of temporal speech features promises to scale second-language (L2) fluency research, yet the validity of such systems must be established at component and measure levels across languages and tasks. We developed and validated a shared English--Japanese pipeline that computes nine clause-based utterance fluency measures from audio via ASR, disfluency-aware clause segmentation, and neural filler scoring. Evaluation followed a staged design: component-level agreement for clause boundaries and pause-location classification, followed by measure-level concurrent validity. For English ($N=40$), automatic clause boundaries showed almost perfect agreement with gold references (micro $F1=.845$, $\kappa=.816$), and automatic MCP/ECP pause-location labels agreed almost perfectly with gold labels ($\kappa=.840$, accuracy $=.921$). Concurrent validity was strong across all nine measures for both English ($N=39$; mean Pearson $r=.936$) and Japanese ($N=40$; mean Pearson $r=.953$). These results demonstrate the feasibility of a cross-lingual, clause-based automatic fluency annotation pipeline that substantially exceeds prior benchmark agreement levels, while identifying pause-duration measures as the most sensitive to pipeline error.
\end{abstract}

\noindent\textbf{Keywords:} L2 speech, utterance fluency, automatic annotation, clause segmentation, ASR, cross-lingual validation, English, Japanese

%% ====================================================================
%% 1. INTRODUCTION
%% ====================================================================
\section{Introduction}

\subsection{Objective measurement of L2 oral fluency}

Objective fluency measurement is central to second language (L2) speech research and assessment. Utterance fluency (UF) --- the temporal characteristics of speech production --- is conventionally decomposed into three dimensions: speed fluency (SF), breakdown fluency (BDF), and repair fluency (RF) \citep{TavakoliSkehan2005}. SF reflects the pace and density of information delivery and is closely related to cognitive processing speed \citep{SuzukiKormos2023}. BDF captures pausing behavior and reflects disruptions in speech production processing; crucially, mid-clause pauses (MCPs) and end-clause pauses (ECPs) carry distinct psycholinguistic significance, as MCPs more strongly predict listener-based fluency judgments than ECPs \citep{deJong2016, SuzukiEtAl2021}. RF is associated with disfluency phenomena such as repetitions, self-corrections, and false starts \citep{Kormos2006}.

A recent meta-analysis of the relationship between UF measures and perceived fluency (PF) ratings confirmed that speed and composite measures are most strongly associated with fluency judgments ($r = |.62$--$.76|$), followed by pause frequency ($r = .59$) and pause duration ($r = .46$), while RF measures show a weaker but significant association ($r = .20$) \citep{SuzukiEtAl2021}. Furthermore, the relationship is moderated by pause location: MCPs contribute more strongly to perceived fluency prediction ($r = .72$) than ECPs ($r = .48$). These findings underscore the importance of computing a comprehensive set of clause-based UF measures --- including location-specific pause metrics --- rather than relying on global temporal indicators alone.

A persistent methodological challenge concerns the unit of analysis for BDF. The clause is the standard unit for classifying pause location \citep{FosterEtAl2000}, yet operational definitions vary considerably across studies. \citet{VercellottiHall2024} argued for a broader clause framework in L2 research that encompasses not only finite clauses but also coordinated verb phrases with complements, nonfinite clauses with overt elements, and copula-less predicates. Their ``verb + element'' principle provides a principled and replicable standard: a verbal construction reaches clause status when the verb has a complement or adjunct. The present pipeline operationalizes this framework explicitly.

\subsection{Automatic annotation of temporal features}

Despite the usefulness of objective UF measures, manual annotation of temporal features --- pauses, disfluency words, clause boundaries --- is highly labor-intensive, which limits the scalability of fluency research \citep{deJongWempe2009}. Acoustic processing approaches can detect syllable nuclei and silent pauses automatically \citep{deJongWempe2009, deJongEtAl2021}, and recent work has extended these to filled pause detection with acceptable accuracy \citep{deJongEtAl2021, Rose2020}. However, purely acoustic approaches cannot determine pause location relative to clause boundaries and cannot compute RF measures, both of which require written transcriptions and syntactic analysis.

Machine learning (ML) techniques have therefore been incorporated into temporal feature annotation systems. \citet{ChenYoon2011} proposed an NLP-based method to detect clause boundaries and disfluency onsets, achieving $F1 = .690$ for clause boundary detection using ASR transcriptions. \citet{Matsuura2022} refined the cascade approach by integrating BERT-based disfluency detection with dependency parsing for clause boundary annotation and pause-location classification, reporting $\kappa = .674$ and $\kappa = .613$ for these two tasks respectively. More recently, \citet{Matsuura2025} validated the robustness of their system across speaking tasks and proficiency levels, finding substantial agreement for monologic tasks ($\kappa = .635$--$.749$ for pause location) but lower accuracy in dialogic settings. These studies demonstrate the potential of cascaded ML systems while highlighting persistent challenges: task sensitivity, error propagation across modules, and the predominance of English-only validation.

Error propagation is a fundamental concern in cascaded annotation systems. ASR errors can disrupt syntactic structure, which subsequently lowers clause boundary accuracy \citep{KnillEtAl2018, KnillEtAl2019}. \citet{KnillEtAl2019} showed that lower-proficiency L2 learners produce more grammatical errors, increasing ASR word error rates (A1 level: WER $= 39.0\%$; C level: WER $= 21.0\%$), with downstream effects on grammatical error detection. Annotation systems should therefore be evaluated under realistic error propagation conditions --- testing component-level agreement alongside measure-level correlation --- rather than validating only end-to-end correlations.

\subsection{Gaps and motivation}

Three gaps motivate the present study. First, no validated cross-lingual pipeline exists for clause-based fluency annotation. Existing systems have been developed and tested exclusively in English, leaving open whether a shared processing architecture can produce valid measures across typologically different languages. English and Japanese present contrasting challenges: different word orders (SVO vs.\ SOV), subject expression (overt vs.\ pro-drop), and clause-chaining structures (coordination vs.\ te-form chains), all of which bear on clause segmentation.

Second, prior validation studies have typically reported measure-level correlations without component-level checks on intermediate outputs. High end-to-end correlations can mask weak boundary or pause-label agreement, especially if errors cancel in aggregation. A staged evaluation design --- from component-level agreement to measure-level validity --- provides more diagnostic evidence for system quality.

Third, operational clause definitions in automated systems remain under-specified. Prior work has relied on implicit parser decisions or corpus-specific rules without explicit reference to a theoretical framework. The present pipeline operationalizes the Vercellotti \& Hall clause framework at the script level, making clause-coding decisions inspectable and replicable.

\subsection{The current study}

To address these gaps, we developed a shared English--Japanese pipeline for automatic clause-based fluency annotation and evaluated it with a staged design. For each language, we address three research questions:

\begin{description}[leftmargin=2.2cm]
\item[RQ1] \textit{Pause-location agreement}: To what extent do MCP/ECP pause labels from the automatic pipeline agree with gold-standard labels?
\item[RQ2] \textit{Clause-segmentation agreement}: To what extent do automatically generated clause boundaries agree with human-annotated clause boundaries?
\item[RQ3] \textit{Concurrent validity}: To what extent do automatically computed CAF measures correlate with manually referenced CAF measures?
\end{description}

In this release, all three English questions and Japanese RQ3 are complete. Japanese RQ1 and RQ2 analyses are currently in progress and will be reported in the final version. Table~\ref{tab:rq_status} summarizes the completion status.

\begin{table}[ht]
\centering
\begin{tabular}{lll}
\toprule
Language & Question & Status \\
\midrule
English & RQ1 (pause-location agreement) & Complete \\
English & RQ2 (clause-segmentation agreement) & Complete \\
English & RQ3 (concurrent validity) & Complete \\
Japanese & RQ1 (pause-location agreement) & In progress \\
Japanese & RQ2 (clause-segmentation agreement) & In progress \\
Japanese & RQ3 (concurrent validity) & Complete \\
\bottomrule
\end{tabular}
\caption{Completion status of the six research questions.}
\label{tab:rq_status}
\end{table}

All pipeline scripts, trained models, gold annotations, and analysis outputs are publicly available as an open release bundle at \texttt{[URL\_TO\_BE\_INSERTED]}.

%% ====================================================================
%% 2. METHOD
%% ====================================================================
\section{Method}

\subsection{Data}

\subsubsection{English corpus}

The English corpus consists of 40 speech files (20 ST1 picture narrative, 20 ST2 argumentative) selected from the ALLSSTAR corpus (Archive of L1 and L2 Scripted and Spontaneous Transcripts and Recordings). Speakers represent diverse L1 backgrounds, providing a range of proficiency levels and accent patterns. Each speech sample is approximately two minutes in duration. The selected files are documented in the release artifact \texttt{selected\_files.json}.

\subsubsection{Japanese corpus}

The Japanese corpus consists of 40 speech files (20 ST1, 20 ST2) from an L2 Japanese speech dataset. The manual gold-standard clause annotations were independently produced and finalized as the \texttt{manual\_clauses\_gold\_v2} set. File identifiers and task assignments are documented in the release artifacts.

\subsubsection{Gold-reference construction (English)}
\label{sec:gold}

Gold-standard clause boundary annotations for the English corpus were constructed via an LLM-assisted workflow adapted from \citet{MorinLarsson2025}:

\begin{enumerate}[leftmargin=*]
\item Two trained coders independently annotated clause boundaries on 10 randomly selected blind files.
\item Disagreements were adjudicated to produce a gold reference for these 10 files.
\item A large language model (Claude) was trained on the adjudicated data and evaluated on 5 locked test files, achieving micro $F1 = .929$ and $\kappa = .914$.
\item The model annotated 30 production files, and outputs were manually reviewed and accepted as gold by the first author.
\end{enumerate}

This yielded 40 gold files (20 ST1, 20 ST2). For RQ3 concurrent validity, one file (\texttt{ALL\_139\_M\_PBR\_ENG\_ST1}) was excluded due to a manual preamble mismatch, yielding a quality-filtered cohort of $N = 39$ (ST1 = 19, ST2 = 20).

\subsection{Pipeline architecture}

The shared release pipeline processes L2 speech through five stages:

\begin{center}
\fbox{Audio} $\rightarrow$ \fbox{ASR + word alignment} $\rightarrow$ \fbox{Disfluency-aware clause segmentation} $\rightarrow$ \fbox{Neural filler scoring} $\rightarrow$ \fbox{CAF computation}
\end{center}

Both languages follow the same high-level architecture, with language-specific adaptations at each stage. Three design constraints determined the implementation: (a) clause-based CAF measures require precise word timing, not only transcript text; (b) one open stack must be usable in both English and Japanese within the same release; and (c) clause coding must be explicit, reproducible, and grounded in the Vercellotti \& Hall framework.

\subsubsection{Stage 1: ASR and word-time alignment}

We employ Qwen3-ASR (1.7B parameters) as the open-source multilingual ASR front end, paired with Qwen3 Forced Aligner (0.6B parameters) for rough word-level timestamps. ASR is run with a disfluency-preserving prompt that encourages the model to transcribe fillers and hesitations rather than suppressing them, since accurate filler capture is critical for downstream pause metrics. To improve timestamp precision, we subsequently apply Montreal Forced Aligner (MFA) with high beam settings (beam $= 100$, retry beam $= 400$).

A key technical innovation is \textit{filler-augmented alignment}: placeholder filler tokens (e.g., ``uh'' for English) are injected into inter-word gaps exceeding 400 ms before MFA runs. The number of injected fillers is determined by $k = \lfloor(\mathit{gap} - 0.35) / 0.55\rfloor + 1$, capped at 3 per gap. These placeholders are absorbed during forced alignment and discarded in the map-back step. This technique prevents MFA from distributing gap durations across adjacent word boundaries, which would otherwise compress pause onset/offset times and distort downstream pause measures. For English, the \texttt{english\_us\_arpa} acoustic model is used; for Japanese, the \texttt{japanese\_mfa} model with deterministic filler token pools of varying length (short, medium, long Japanese fillers).

Japanese ASR includes an additional MeCab re-tokenization step (via \texttt{fugashi}) before MFA to split merged tokens and stabilize downstream token mapping.

\subsubsection{Stage 2: Disfluency detection}

Both clause segmenters load a shared neural disfluency detector before clause parsing. The model is a fine-tuned \texttt{XLM-RoBERTa-base} token classifier (two labels: fluent/disfluent), trained on a combination of real English disfluency data from Switchboard \citep{ZayatsEtAl2019} and synthetic English/Japanese disfluency data (88.5K training sentences total), following the synthetic data augmentation approach described by \citet{Kundu2022}. Training parameters included 3 epochs, learning rate $2 \times 10^{-5}$, and batch size 16.

The detector serves a pruning function: identified disfluent tokens are suppressed before or during clause assembly. This standardizes syllable/mora counts for rate-based measures \citep{SuzukiRevesz2023} and reduces noise in the dependency parse input for clause segmentation. The Japanese segmenter additionally applies deterministic post-rules (repeated-token collapse, elongated-form handling, split-repetition fixes) after neural prediction to improve robustness on conversational artifacts.

\subsubsection{Stage 3: Clause segmentation}

Both segmenters first apply sentence segmentation using wtpsplit (Segment any Text), a neural sentence boundary detector suitable for unpunctuated L2 speech. Sentence boundaries are enforced as clause boundaries. Each sentence is then dependency-parsed, and clause heads are identified and classified.

\paragraph{English.}
The English segmenter applies a rule layer over spaCy dependency parses (transformer model), explicitly operationalizing the ``verb + element'' clause logic of \citet{VercellottiHall2024}. A token is considered a potential clause head if it carries a verb-like POS tag (VB*, VERB, AUX) and a clausal dependency relation. The ``verb + element'' check tests whether the verb has at least one qualifying dependent among complements (\texttt{obj}, \texttt{iobj}, \texttt{ccomp}, \texttt{xcomp}, \texttt{attr}), oblique arguments (\texttt{obl}), and adjuncts (\texttt{advmod}, \texttt{prep}). The segmenter classifies the following constructions:

\begin{itemize}[leftmargin=*, nosep]
\item \textit{Independent clauses:} ROOT-level finite verbs.
\item \textit{Subordinate clauses:} \texttt{advcl}, \texttt{ccomp}, \texttt{acl}, \texttt{acl:relcl}, \texttt{csubj} relations.
\item \textit{Coordinated VPs:} \texttt{conj} of a verb, but only if the conjunct has its own complement or adjunct (stricter than Vercellotti's inclusive approach for shared complements).
\item \textit{Nonfinite clauses:} \texttt{xcomp} and participial constructions, with Vercellotti's ``verb + element'' requirement.
\item \textit{Minor clauses:} Stance verbs (e.g., \textit{I think}, \textit{I believe}) taking a complement clause, tagged separately for analysis.
\item \textit{Copula-less predicates:} ROOT-level ADJ/NOUN without overt copula (common in L2 speech).
\end{itemize}

Clause spans are collected via subtree traversal (excluding other clause-head subtrees) and aligned back to TextGrid word intervals for timing.

\paragraph{Japanese.}
The Japanese segmenter uses GiNZA (\texttt{ja\_ginza\_electra}) with a Japanese Universal Dependencies rule layer adapted to the same theoretical principle. Predicate detection includes VERB, ADJ, and NOUN tokens; misparsed proper nouns tagged as verbs are filtered. A key language-specific adaptation concerns \textit{te}-form verb chains, which are pervasive in Japanese speech. Following the Vercellotti ``verb + element'' rule, a \textit{te}-form verb receives clause status only if it has its own complement or adjunct; bare \textit{te}-forms are merged with the following clause. The complement check uses a strict set (\texttt{obj}, \texttt{iobj}, \texttt{obl}, \texttt{advmod}, \texttt{nmod}), deliberately excluding \texttt{nsubj} because shared and implied subjects are common in Japanese. The segmenter also handles explicit subordinators (\textit{kara} `because', \textit{kedo} `although', \textit{node} `because', \textit{ba} `if'), \textit{tari}-form chains, and the clausal subject (\texttt{csubj}) construction.

\paragraph{Why explicit rules?}
Prior L2 fluency studies often under-specify operational clause decisions, making downstream pause and CAF measures difficult to compare across studies. We therefore expose all clause-coding decisions in inspectable, script-level rules grounded in \citet{VercellottiHall2024}, improving reproducibility and enabling principled cross-lingual comparison.

\subsubsection{Stage 4: Neural filler candidate scoring}

Modern ASR systems often omit low-energy fillers and disfluencies from transcriptions. If undetected, these speech regions are over-counted as pure silence, biasing pause metrics. We address this with a post-ASR acoustic filler detector that scores candidate filler regions within ASR-detected gaps.

The gap-only neural filler classifier uses a TC-ResNet8-style temporal CNN over per-clip normalized log-mel spectral features ($1 \times 64 \times 101$). The model was trained on the PodcastFillers dataset \citep{ZhuEtAl2022} with binary labeling (positive: uh/um; negative: words/breath/laughter/music), achieving validation $F1 = .941$ and test $F1 = .933$. At inference, each ASR gap of sufficient length is scored against a threshold of 0.50; accepted filler candidates are used to split or suppress pause intervals before CAF computation.

\subsubsection{Stage 5: CAF computation}

CAF calculators compute nine measures from clause-segmented, filler-adjusted TextGrids (Table~\ref{tab:measures}). The classifier refinement step operates as follows: for each pause interval ($\geq 250$ ms), any overlap with predicted filler-speech islands is subtracted, and the remaining silence segments are retained as pauses only if they individually meet the 250 ms threshold. Each resulting pause is then classified as MCP or ECP: a pause is labeled ECP if its onset falls within 150 ms of any clause offset; otherwise it is labeled MCP. Pauses that fall outside all clause boundaries (e.g., before the first clause or after the last) default to ECP.

English uses syllable-based normalization; syllable counts are derived from the phone tier when available, or estimated heuristically from orthographic form. Japanese uses mora-based normalization.

\begin{table}[ht]
\centering
\small
\begin{tabular}{llp{7.5cm}}
\toprule
Type & Measure & Description \\
\midrule
Speed & AR & Articulation rate: syllables (morae) per phonation time \\
\addlinespace
Composite & SR & Speech rate: syllables (morae) per total speech time \\
         & MLR & Mean length of run: mean syllables (morae) between pauses \\
\addlinespace
\multirow{4}{*}{Breakdown (freq.)} & MCPR & Mid-clause pause ratio: MCPs per syllable (mora) \\
 & ECPR & End-clause pause ratio: ECPs per syllable (mora) \\
 & PR & Pause ratio: all pauses per syllable (mora) \\
\addlinespace
\multirow{3}{*}{Breakdown (dur.)} & MCPD & Mean mid-clause pause duration (s) \\
 & ECPD & Mean end-clause pause duration (s) \\
 & MPD & Mean pause duration (s) \\
\bottomrule
\end{tabular}
\caption{Nine CAF measures computed by the pipeline. Minimum pause threshold: 250 ms. English rates are syllable-normalized; Japanese rates are mora-normalized.}
\label{tab:measures}
\end{table}

\subsection{Evaluation methods}

\subsubsection{RQ2: Clause-segmentation agreement}

Clause agreement was evaluated as per-word binary boundary classification after minimum-edit-distance alignment between canonical (manual transcript) and ASR token sequences, replicating the alignment logic of NIST's SCTK tool \citep{ChenYoon2012}. At each alignment position:

\begin{itemize}[leftmargin=*]
\item \textbf{Correct/Substitution:} gold and auto boundary labels are compared directly at the aligned position.
\item \textbf{Deletion} (manual-only word): auto label is set to 0 (pipeline penalized for missing word).
\item \textbf{Insertion} (ASR-only word): gold label is set to 0.
\end{itemize}

This follows the strict speech-evaluation alignment logic of \citet{ChenYoon2012} rather than longest-common-subsequence matching, which can produce optimistically inflated agreement by skipping substitution sites. Metrics include micro and macro precision, recall, $F1$, and Cohen's $\kappa$ \citep{LandisKoch1977}.

\subsubsection{RQ1: Pause-location agreement}

For each silent pause $\geq 250$ ms in the automatic words tier, two MCP/ECP labels were obtained: (a) the automatic label from auto clause intervals, and (b) a gold label derived by projecting gold clause boundaries onto auto word timing via the edit-distance alignment from RQ2. A pause was classified as ECP if its onset fell within 150 ms of any clause offset; otherwise MCP. Metrics include Cohen's $\kappa$, accuracy, and per-class precision, recall, and $F1$.

The strength of agreement in terms of Cohen's $\kappa$ was interpreted following \citet{LandisKoch1977}: $< .20$ slight, $.21$--$.40$ fair, $.41$--$.60$ moderate, $.61$--$.80$ substantial, $> .80$ almost perfect.

\subsubsection{RQ3: Concurrent validity}

For each of the nine CAF measures, we computed Pearson $r$, Spearman $\rho$, ICC(2,1), and mean absolute error (MAE) between automatic and manual outputs. English uses the quality-filtered cohort ($N = 39$; see Section~\ref{sec:gold}); Japanese uses $N = 40$.

\subsubsection{Sample size considerations}

For RQ1 and RQ2, the units of analysis are individual pause events ($N = 1{,}902$) and boundary positions ($N = 1{,}131$), respectively. According to power analyses for Cohen's $\kappa$, the minimum sample sizes to detect substantial agreement ($\kappa > .61$) with power $\beta = .80$ are 151 for binary annotation tasks and 67 for two-category classification \citep{DonnerEliasziw1987}. Both RQ1 and RQ2 substantially exceed these thresholds. For RQ3 concurrent validity, the unit of analysis is the speech file ($N = 39$ for English, $N = 40$ for Japanese). Using standard power tables for bivariate correlations (two-tailed $\alpha = .05$, $\beta = .80$), the minimum sample size to detect a large correlation ($r \geq .60$; \citealp{PlonskyOswald2014}) is approximately $N = 19$. Both cohorts exceed this threshold.

%% ====================================================================
%% 3. RESULTS
%% ====================================================================
\section{Results}

\subsection{English RQ2: Clause-segmentation agreement}

Table~\ref{tab:en_rq2} presents clause boundary agreement between automatic and gold-standard annotations across the 40 English files.

\begin{table}[ht]
\centering
\begin{tabular}{lcccccc}
\toprule
Subset & Files & Gold boundaries & Precision & Recall & $F1$ (micro) & $\kappa$ (micro) \\
\midrule
Overall & 40 & 1{,}131 & .848 & .842 & .845 & .816 \\
ST1 & 20 & 496 & .882 & .857 & .869 & .845 \\
ST2 & 20 & 635 & .822 & .830 & .826 & .795 \\
\bottomrule
\end{tabular}
\caption{English clause-boundary agreement (automatic vs.\ gold).}
\label{tab:en_rq2}
\end{table}

Overall $\kappa = .816$ indicates almost perfect agreement \citep{LandisKoch1977}. Macro means were $F1 = .846$ ($SD = .093$) and $\kappa = .819$ ($SD = .102$). Per-file $F1$ ranged from .618 to 1.000 (median .859). Mean alignment WER between canonical and ASR clause text was .121 (12.1\%). Precision and recall were well balanced, indicating no systematic over- or under-segmentation.

ST1 (picture narrative) yielded slightly higher agreement than ST2 (argumentative), consistent with lower ASR error rates on structured narrative speech \citep{KnillEtAl2018}. The two files with lowest per-file agreement ($\kappa < .65$) both had WER $> .19$, confirming that ASR errors propagate to downstream clause boundary detection.

\subsection{English RQ1: Pause-location agreement}

Table~\ref{tab:en_rq1} presents pause-location classification agreement.

\begin{table}[ht]
\centering
\begin{tabular}{lcccccc}
\toprule
Subset & Files & Pauses & $\kappa$ & Accuracy & MCP $F1$ & ECP $F1$ \\
\midrule
Overall & 40 & 1{,}902 & .840 & .921 & .929 & .912 \\
ST1 & 20 & 822 & .873 & .937 & .941 & .932 \\
ST2 & 20 & 1{,}080 & .815 & .909 & .920 & .894 \\
\bottomrule
\end{tabular}
\caption{English pause-location agreement (automatic vs.\ gold MCP/ECP labels).}
\label{tab:en_rq1}
\end{table}

Overall $\kappa = .840$ indicates almost perfect agreement. MCP classification ($F1 = .929$) slightly outperformed ECP ($F1 = .912$), as expected: mid-clause pauses are more clearly positioned within clause boundaries, while end-clause pauses near clause offsets are sensitive to boundary placement. Per-file accuracy ranged from .789 to 1.000 (median .927, $SD = .052$); six files achieved perfect accuracy.

The pattern across tasks parallels RQ2: ST1 accuracy exceeds ST2. Notably, the file with lowest RQ1 accuracy (.789) also had the lowest RQ2 agreement ($\kappa = .578$), confirming the error propagation chain from ASR errors through clause boundaries to pause classification --- the fundamental cascading concern identified by \citet{KnillEtAl2018, KnillEtAl2019}.

\subsection{English RQ3: Concurrent validity ($N = 39$)}

Table~\ref{tab:en_rq3} presents overall concurrent validity for the quality-filtered English cohort.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Measure & Pearson $r$ & Spearman $\rho$ & ICC(2,1) & MAE \\
\midrule
AR & .956 & .911 & .953 & 0.157 \\
SR & .988 & .978 & .984 & 0.082 \\
MLR & .971 & .968 & .965 & 0.454 \\
MCPR & .966 & .981 & .962 & 0.011 \\
ECPR & .864 & .889 & .866 & 0.008 \\
PR & .961 & .965 & .958 & 0.014 \\
MCPD & .821 & .808 & .799 & 0.090 \\
ECPD & .938 & .912 & .935 & 0.115 \\
MPD & .957 & .908 & .944 & 0.074 \\
\bottomrule
\end{tabular}
\caption{English RQ3 overall concurrent validity (quality-filtered cohort, $N = 39$).}
\label{tab:en_rq3}
\end{table}

Overall English correlations span $.821$--$.988$ (mean $r = .936$). All nine Pearson correlations are large ($r > .60$; \citealp{PlonskyOswald2014}), and all ICC values exceed .79, indicating good to excellent absolute agreement.

By task, the picture narrative (ST1, $N = 19$) is most challenging for MCPD ($r = .713$, ICC $= .669$, MAE $= 0.097$), while the argumentative task (ST2, $N = 20$) remains strong across all measures (minimum $r = .910$ for MCPD). ST1 also shows relatively lower ECPR agreement ($r = .774$, ICC $= .775$). ST2 is uniformly strong, with the weakest measure being ECPR ($r = .935$). Table~\ref{tab:en_rq3_tasks} presents the task-level breakdown.

\begin{table}[ht]
\centering
\small
\begin{tabular}{l cccc cccc}
\toprule
 & \multicolumn{4}{c}{ST1 ($N = 19$)} & \multicolumn{4}{c}{ST2 ($N = 20$)} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
Measure & $r$ & $\rho$ & ICC & MAE & $r$ & $\rho$ & ICC & MAE \\
\midrule
AR   & .958 & .916 & .958 & 0.153 & .950 & .860 & .948 & 0.161 \\
SR   & .991 & .981 & .987 & 0.083 & .985 & .985 & .983 & 0.082 \\
MLR  & .969 & .965 & .961 & 0.434 & .972 & .962 & .968 & 0.472 \\
MCPR & .966 & .984 & .965 & 0.013 & .977 & .974 & .960 & 0.010 \\
ECPR & .774 & .781 & .775 & 0.010 & .935 & .956 & .935 & 0.006 \\
PR   & .954 & .966 & .952 & 0.015 & .977 & .956 & .965 & 0.013 \\
MCPD & .713 & .741 & .669 & 0.097 & .910 & .865 & .886 & 0.084 \\
ECPD & .916 & .905 & .904 & 0.141 & .963 & .920 & .965 & 0.092 \\
MPD  & .934 & .868 & .920 & 0.093 & .981 & .958 & .970 & 0.056 \\
\bottomrule
\end{tabular}
\caption{English RQ3 concurrent validity by task.}
\label{tab:en_rq3_tasks}
\end{table}

\subsection{Japanese RQ3: Concurrent validity ($N = 40$)}

Table~\ref{tab:ja_rq3} presents overall concurrent validity for the Japanese cohort.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Measure & Pearson $r$ & Spearman $\rho$ & ICC(2,1) & MAE \\
\midrule
AR & .947 & .935 & .903 & 0.242 \\
SR & .992 & .988 & .982 & 0.116 \\
MLR & .991 & .987 & .975 & 0.460 \\
MCPR & .912 & .933 & .913 & 0.009 \\
ECPR & .903 & .883 & .898 & 0.008 \\
PR & .984 & .978 & .978 & 0.009 \\
MCPD & .955 & .902 & .878 & 0.151 \\
ECPD & .942 & .902 & .884 & 0.231 \\
MPD & .948 & .876 & .880 & 0.184 \\
\bottomrule
\end{tabular}
\caption{Japanese RQ3 overall concurrent validity ($N = 40$).}
\label{tab:ja_rq3}
\end{table}

Japanese overall correlations span $.903$--$.992$ (mean $r = .953$), slightly exceeding the English mean. All nine measures show large positive correlations, confirming strong concurrent validity for the Japanese pipeline.

By task, both ST1 and ST2 show consistently strong correlations (Table~\ref{tab:ja_rq3_tasks}). The weakest task-specific value is ECPR in ST1 ($r = .872$), still well above the large-effect threshold. Unlike the English results, Japanese MCPD is uniformly strong across tasks ($r = .959$ in ST1, $r = .953$ in ST2), suggesting that the Japanese corpus may have a more stable MCP distribution.

\begin{table}[ht]
\centering
\small
\begin{tabular}{l cccc cccc}
\toprule
 & \multicolumn{4}{c}{ST1 ($N = 20$)} & \multicolumn{4}{c}{ST2 ($N = 20$)} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
Measure & $r$ & $\rho$ & ICC & MAE & $r$ & $\rho$ & ICC & MAE \\
\midrule
AR   & .965 & .965 & .922 & 0.227 & .929 & .911 & .886 & 0.257 \\
SR   & .993 & .989 & .986 & 0.101 & .993 & .976 & .979 & 0.132 \\
MLR  & .997 & .987 & .988 & 0.348 & .986 & .955 & .961 & 0.572 \\
MCPR & .895 & .884 & .898 & 0.009 & .925 & .925 & .928 & 0.009 \\
ECPR & .872 & .838 & .861 & 0.009 & .921 & .892 & .921 & 0.008 \\
PR   & .987 & .986 & .982 & 0.007 & .981 & .927 & .977 & 0.010 \\
MCPD & .959 & .863 & .877 & 0.130 & .953 & .920 & .882 & 0.173 \\
ECPD & .932 & .932 & .865 & 0.250 & .947 & .880 & .896 & 0.213 \\
MPD  & .957 & .913 & .889 & 0.174 & .940 & .872 & .875 & 0.195 \\
\bottomrule
\end{tabular}
\caption{Japanese RQ3 concurrent validity by task.}
\label{tab:ja_rq3_tasks}
\end{table}

\subsection{Japanese RQ1 and RQ2 (in progress)}

Japanese pause-location and clause-boundary agreement analyses are currently in progress using the same metric suites as English. These will be reported with full task-level breakdowns in the final manuscript version.

\subsection{Comparison with prior systems}

Table~\ref{tab:comparison} places the current English results alongside published benchmarks.

\begin{table}[ht]
\centering
\small
\begin{tabular}{llcc}
\toprule
System & Task & Pause loc.\ $\kappa$ & Clause $F1$ \\
\midrule
\citet{ChenYoon2011} & Monologue (L2 EN) & --- & .690 \\
\citet{Matsuura2025} & Monologue (L2 EN) & .613--.749 & --- \\
\citet{Matsuura2025} & Dialogue (L2 EN) & .596--.672 & --- \\
\textbf{Current pipeline} & \textbf{Monologue (L2 EN)} & \textbf{.840} & \textbf{.845} \\
\bottomrule
\end{tabular}
\caption{Comparison with prior automatic annotation systems. Direct comparison should be interpreted with caution due to differences in corpora, L1 backgrounds, proficiency levels, and gold-standard construction.}
\label{tab:comparison}
\end{table}

The current pipeline substantially exceeds the pause-location agreement reported by \citet{Matsuura2025} ($\kappa = .840$ vs.\ $.613$--$.749$) and the clause boundary detection accuracy of \citet{ChenYoon2011} ($F1 = .845$ vs.\ $.690$). These improvements likely reflect advances in ASR quality (Qwen3 vs.\ Rev.ai/Wav2Vec2 forced alignment), MFA-based timestamp refinement, a modern dependency parser (spaCy transformer), and the filler-augmented alignment technique. However, corpus, population, and gold-standard differences preclude strong causal claims about any single component.

%% ====================================================================
%% 4. DISCUSSION
%% ====================================================================
\section{Discussion}

\subsection{Component-level agreement exceeds prior benchmarks}

English clause boundary agreement ($\kappa = .816$, $F1 = .845$) reached the ``almost perfect'' range \citep{LandisKoch1977} and substantially exceeded the clause boundary $F1$ reported by \citet{ChenYoon2011} ($F1 = .690$). Pause-location classification ($\kappa = .840$) likewise surpassed the $.613$--$.749$ range reported for monologic L2 English by \citet{Matsuura2025}. These results are noteworthy because the current evaluation uses a stricter alignment method (edit-distance rather than longest-common-subsequence) and a broader clause definition \citep{VercellottiHall2024} that includes more boundary types than traditional definitions.

The component-level results also provide an important diagnostic that end-to-end correlations alone cannot offer. High measure-level correlations can arise even when intermediate boundary or pause-label agreement is moderate, if errors cancel in aggregation. By demonstrating strong agreement at both the boundary and pause-label levels, the present analysis reduces the risk that the strong RQ3 correlations are artifacts of error cancellation rather than genuine annotation quality.

\subsection{Error propagation from ASR through clause boundaries to pause classification}

A clear error propagation chain was observed across pipeline stages. Files with WER $> .20$ showed noticeably lower boundary agreement (mean $\kappa \approx .72$ vs.\ $.84$ for WER $\leq .20$), confirming the findings of \citet{KnillEtAl2018, KnillEtAl2019} that ASR errors propagate to downstream NLP tasks. The file with the lowest clause boundary agreement also had the lowest pause-location accuracy, demonstrating that boundary errors cascade into pause classification.

Task type moderated agreement levels, with ST1 (picture narrative) consistently outperforming ST2 (argumentative) across both RQ1 and RQ2. This aligns with the expectation that structured narrative tasks produce more regular speech with lower ASR error rates, while argumentative speech introduces more complex syntax, longer utterances, and repair sequences that challenge both ASR and clause segmentation. \citet{Matsuura2025} similarly found weaker disfluency detection and pause-location agreement in their dialogic task compared to monologic tasks, attributing this to co-constructive discourse features. The present ST1/ST2 contrast, although less extreme (both tasks are monologic), follows the same pattern on the dimension of cognitive demand \citep{SuzukiKormos2023}.

\subsection{Concurrent validity is strong across languages and measures}

Concurrent validity was strong in both languages: mean Pearson $r = .936$ for English and $.953$ for Japanese. Across both languages, speed and composite measures (SR, MLR, AR) and pause-frequency measures (MCPR, ECPR, PR) were the most stable, while pause-duration measures (MCPD, ECPD, MPD) were comparatively more sensitive. This pattern mirrors the general finding in the literature that composite and speed measures correlate most strongly between automatic and manual systems \citep{Matsuura2025, deJongEtAl2021}.

The relative weakness of MCPD, particularly in English ST1 ($r = .713$, ICC $= .669$), deserves interpretation. MCPD is computed as the \textit{mean} duration of mid-clause pauses; in files with few MCPs, a single falsely detected or misaligned pause can substantially shift the mean. The low frequency of MCPs in fluent speakers' speech makes this measure inherently more vulnerable to individual annotation errors --- an observation also noted by \citet{Matsuura2025}, who found that MCPD correlations varied across proficiency levels, with weaker agreement in high-fluency groups where MCP counts are smallest.

Interestingly, Japanese MCPD showed uniformly strong correlations ($r = .959$ and $.953$ for ST1 and ST2), unlike the English pattern. This may reflect differences in pausing behavior between the two language cohorts, or alternatively, may result from the Japanese corpus having a more stable MCP distribution. Cross-lingual differences in MCP--ECPD sensitivity warrant further investigation with matched proficiency samples.

\subsection{Cross-lingual transportability of the pipeline}

The shared architecture produces valid results in both English and Japanese, demonstrating cross-lingual transportability with explicit language-specific adaptations. Three adaptations deserve highlighting. First, mora-based normalization in Japanese replaces syllable-based normalization in English, reflecting the fundamental prosodic unit difference. Second, the Japanese clause segmenter adapts the Vercellotti ``verb + element'' framework to handle te-form chains --- a construction without English parallel --- by granting clause status only to te-form verbs with their own complement or adjunct. Third, the Japanese pipeline includes MeCab re-tokenization and a robust map-back algorithm to handle tokenization mismatches between ASR and MFA, a challenge less prominent in English.

Despite these adaptations, the core architecture remains shared: the same ASR model, the same disfluency detector, the same filler classifier, and the same clause-coding principle. The strong Japanese RQ3 results suggest that this shared-architecture approach is viable for cross-lingual deployment, at least for the English--Japanese pair evaluated here.

\subsection{Three key innovations}

Beyond the quantitative results, three design innovations underlying the pipeline deserve emphasis, as they address structural weaknesses in prior cascaded annotation systems.

\textit{Precise word-level timestamps via forced alignment.} In a cascaded pipeline, all downstream annotations --- clause boundaries, pause locations, CAF measures --- depend on accurate word-level timing. If timestamps are imprecise, errors propagate through every subsequent stage (garbage in, garbage out). Prior systems relied on ASR-internal timestamps or simple Wav2Vec2 forced alignment, which can be imprecise for L2 speech with long pauses and varied pronunciation. We address this by applying Montreal Forced Aligner with high beam settings after Qwen3-ASR, combined with a filler-augmented alignment technique that injects placeholder tokens into gaps before alignment. This prevents the aligner from distributing pause durations across adjacent word boundaries, yielding more accurate pause onset and offset times. The improvement in boundary and pause-location agreement over prior benchmarks (Table~\ref{tab:comparison}) likely reflects, in part, this timestamp quality improvement.

\textit{Dual filler handling.} Modern large-vocabulary ASR models (including Qwen3-ASR) tend to produce clean transcripts by suppressing fillers and hesitations. This is desirable for transcription accuracy but problematic for fluency annotation: unrecognized fillers result in their speech intervals being counted as silent pauses, inflating pause metrics. We address this at two levels. First, at the ASR stage, a disfluency-preserving prompt encourages the model to retain fillers in the transcript. Second, a post-ASR neural filler classifier (trained on the PodcastFillers dataset) scores candidate speech regions within ASR-detected gaps, suppressing or splitting pause intervals where filler speech is detected. This dual approach reduces the systematic over-counting of pauses that would otherwise bias breakdown fluency measures.

\textit{Zero-shot cross-lingual disfluency detection.} Disfluency-annotated speech corpora are concentrated in English (e.g., Switchboard), while Japanese and other languages lack comparable resources. We address this data scarcity by training the disfluency detector on a combination of real English data and synthetic disfluency data generated for both English and Japanese, following the data augmentation approach of \citet{Kundu2022}. Since the underlying model (XLM-RoBERTa) supports 100+ languages, this approach is potentially extensible to other languages without requiring language-specific annotated corpora --- a practical advantage for scaling the pipeline beyond the English--Japanese pair validated here.

\subsection{Implications for fluency research and assessment}

The present pipeline can reduce annotation cost while preserving strong agreement and correlation on the evaluated cohorts. Practically, this makes clause-based CAF analyses feasible at larger scales and supports transparent hybrid workflows where automatic annotation is combined with targeted manual checking. The staged evaluation design --- component-level agreement followed by measure-level validity --- provides a template that future automatic annotation studies should adopt, in line with recommendations to test systems under realistic error propagation constraints \citep{KnillEtAl2018, KnillEtAl2019}.

Methodologically, the explicit operationalization of the Vercellotti \& Hall clause framework \citep{VercellottiHall2024} addresses a longstanding comparability problem in L2 fluency research. By making clause-coding rules inspectable at the script level, researchers can evaluate whether observed differences across studies reflect genuine linguistic phenomena or artifacts of inconsistent clause definitions. The cross-lingual adaptation of this framework --- particularly the principled treatment of Japanese te-form chains --- demonstrates that the ``verb + element'' principle can extend beyond English.

For language assessment contexts, the strong correlations observed for SR, MLR, and PR (the measures most strongly associated with perceived fluency ratings; \citealp{SuzukiEtAl2021}) suggest potential applicability in automated scoring systems. However, the sensitivity of MCPD under certain conditions implies that assessments relying heavily on pause-duration measures should incorporate manual checking, at least until larger validation studies establish robustness across broader populations.

\subsection{Future directions}

Several directions merit investigation. First, the current clause segmenters use explicit, rule-based implementations of the Vercellotti \& Hall framework. While this approach maximizes transparency and interpretability --- every boundary decision traces to an identifiable rule --- it may fail on syntactically unusual L2 constructions that the rule set does not anticipate. A model-based approach (e.g., fine-tuning a sequence labeler on gold clause boundaries) could improve robustness to such cases, at the cost of reduced interpretability and the need for substantial annotated training data per language. Comparing rule-based and model-based clause segmentation on the same gold standard would clarify this trade-off.

Second, external-corpus validation is needed to establish generalizability beyond the cohorts and task types evaluated here. In particular, testing on dialogic speech, lower-proficiency learners, and L1 backgrounds beyond those in the current corpora would strengthen validity claims.

Third, the present study does not include perceived fluency (PF) ratings. Future work should evaluate whether the automatically computed CAF measures predict listener-based fluency judgments at levels comparable to those reported by \citet{Matsuura2025}, which would provide complementary construct validity evidence.

%% ====================================================================
%% 5. LIMITATIONS
%% ====================================================================
\section{Limitations}

Six limitations should guide interpretation.

\begin{enumerate}[leftmargin=*]
\item Japanese RQ1 (pause-location agreement) and RQ2 (clause-boundary agreement) are in progress at this release stage. The complete six-question analysis will be reported in the final manuscript version.

\item Cohort sizes remain moderate (EN $N = 39$ for RQ3; JA $N = 40$), limiting subgroup precision and restricting the power of task-level comparisons. Following \citet{DonnerEliasziw1987}, the minimum sample size to detect substantial agreement ($\kappa > .61$) with $\beta = .8$ varies by base rate, and some per-task cells may be underpowered for detecting moderate agreement differences.

\item English gold boundaries were constructed using an LLM-assisted stage (validated on locked test files at micro $F1 = .929$, $\kappa = .914$, then manually reviewed by the first author). Although this level of LLM accuracy is high and the workflow follows the methods-grade standard of \citet{MorinLarsson2025}, residual model-shaped annotation bias relative to fully independent double-human coding remains possible.

\item External-corpus generalization is not yet established. Current validity claims are specific to the corpora and cohorts evaluated; the pipeline has not been tested on other L1 backgrounds, proficiency levels, or task types (e.g., dialogic tasks, read-aloud conditions).

\item Unlike \citet{Matsuura2025}, the present study does not include perceived fluency (PF) ratings. We therefore cannot evaluate the predictive power of automatic measures for listener-based fluency judgments, which is a complementary dimension of construct validity.

\item The disfluency detector was trained primarily on synthetic and Switchboard data. Although it serves a pruning function rather than producing a reported measure, its errors may still propagate to clause segmentation quality. A comprehensive real-speech disfluency evaluation, particularly for Japanese, remains limited.
\end{enumerate}

%% ====================================================================
%% 6. CONCLUSION
%% ====================================================================
\section{Conclusion}

This study developed and validated a shared English--Japanese pipeline for automatic clause-based fluency annotation, evaluated through a staged design from component-level agreement to measure-level concurrent validity. The English pipeline achieved almost perfect agreement on clause boundaries ($\kappa = .816$) and pause-location classification ($\kappa = .840$), substantially exceeding prior benchmarks. Concurrent validity was strong across all nine CAF measures in both English (mean $r = .936$) and Japanese (mean $r = .953$), with speed and composite measures showing the highest stability and pause-duration measures showing the greatest sensitivity to pipeline error.

Three technical contributions underlie these results. First, filler-augmented forced alignment produces precise word-level timestamps that serve as the foundation for all downstream annotations --- addressing the fundamental garbage-in-garbage-out problem in cascaded systems. Second, dual filler handling at both the ASR and post-ASR stages compensates for the tendency of modern ASR models to suppress fillers, preventing systematic over-counting of silent pauses. Third, zero-shot cross-lingual disfluency detection, trained on synthetic data augmentation, addresses the scarcity of annotated disfluency data in non-English languages.

Beyond these technical contributions, the pipeline advances L2 fluency methodology by providing the first validated cross-lingual architecture for clause-based annotation, demonstrating a staged evaluation design that diagnostically strengthens measure-level validity evidence, and explicitly operationalizing the Vercellotti \& Hall clause framework for transparent and replicable clause coding. All pipeline code, trained models, and analysis scripts are openly available (\texttt{[URL\_TO\_BE\_INSERTED]}) to support replication and extension. The immediate next step is completion of Japanese RQ1--RQ2, which will enable a full six-question cross-lingual analysis.

%% ====================================================================
%% REFERENCES
%% ====================================================================
\begin{thebibliography}{99}

\bibitem[Chen and Yoon(2011)]{ChenYoon2011}
Chen, L., and S.-Y. Yoon. 2011.
Detecting structural events for assessing non-native speech.
In \textit{Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications}, 38--45.

\bibitem[Chen and Yoon(2012)]{ChenYoon2012}
Chen, L., and S.-Y. Yoon. 2012.
Application of structural events detected on ASR outputs for automated speaking assessment.
In \textit{Proceedings of Interspeech 2012}, 767--770.

\bibitem[Chen et~al.(2018)]{ChenEtAl2018}
Chen, L., K. Zechner, S.-Y. Yoon, K. Evanini, X. Wang, A. Loukina, J. Tao, L. Davis, C.~M. Lee, M. Ma, R. Mundkowsky, C. Lu, C.~W. Leong, and B. Gyawali. 2018.
Automated scoring of nonnative speech using the SpeechRater\textsuperscript{SM} v.5.0 engine.
\textit{ETS Research Report Series} 2018(1).

\bibitem[Coulange et~al.(2024)]{CoulangeEtAl2024}
Coulange, S., T. Kato, S. Rossato, and M. Masperi. 2024.
Enhancing language learners' comprehensibility through automated analysis of pause positions and syllable prominence.
\textit{Languages} 9(3): 78.

\bibitem[de Jong(2016)]{deJong2016}
de Jong, N. H. 2016.
Predicting pauses in L1 and L2 speech: The effects of utterance boundaries and word frequency.
\textit{IRAL -- International Review of Applied Linguistics in Language Teaching} 54(2): 113--132.

\bibitem[de Jong and Bosker(2013)]{deJongBosker2013}
de Jong, N. H., and H. R. Bosker. 2013.
Choosing a threshold for silent pauses to measure second language fluency.
In \textit{Proceedings of DiSS 2013}, 17--20.

\bibitem[de Jong and Wempe(2009)]{deJongWempe2009}
de Jong, N. H., and T. Wempe. 2009.
Praat script to detect syllable nuclei and measure speech rate automatically.
\textit{Behavior Research Methods} 41(2): 385--390.

\bibitem[de Jong et~al.(2021)]{deJongEtAl2021}
de Jong, N. H., J. Pacilly, and W. Heeren. 2021.
PRAAT scripts to measure speed fluency and breakdown fluency in speech automatically.
\textit{Assessment in Education: Principles, Policy and Practice} 28(4): 456--476.

\bibitem[Donner and Eliasziw(1987)]{DonnerEliasziw1987}
Donner, A., and M. Eliasziw. 1987.
Sample size requirements for reliability studies.
\textit{Statistics in Medicine} 6(4): 441--448.

\bibitem[Foster et~al.(2000)]{FosterEtAl2000}
Foster, P., A. Tonkyn, and G. Wigglesworth. 2000.
Measuring spoken language: A unit for all reasons.
\textit{Applied Linguistics} 21(3): 354--375.

\bibitem[Knill et~al.(2018)]{KnillEtAl2018}
Knill, K. M., M. J. F. Gales, K. Kyriakopoulos, A. Malinin, A. Ragni, Y. Wang, and A. Caines. 2018.
Impact of ASR performance on free speaking language assessment.
In \textit{Proceedings of Interspeech 2018}, 1641--1645.

\bibitem[Knill et~al.(2019)]{KnillEtAl2019}
Knill, K. M., M. J. F. Gales, P. P. Manakul, and A. P. Caines. 2019.
Automatic grammatical error detection of non-native spoken learner English.
In \textit{Proceedings of ICASSP 2019}, 8127--8131.

\bibitem[Kormos(2006)]{Kormos2006}
Kormos, J. 2006.
\textit{Speech Production and Second Language Acquisition}.
Lawrence Erlbaum Associates.

\bibitem[Kundu et~al.(2022)]{Kundu2022}
Kundu, R., P. Jyothi, and P. Bhattacharyya. 2022.
Zero-shot disfluency detection for Indian languages.
In \textit{Proceedings of the 29th International Conference on Computational Linguistics}, 4442--4453.

\bibitem[Landis and Koch(1977)]{LandisKoch1977}
Landis, J. R., and G. G. Koch. 1977.
The measurement of observer agreement for categorical data.
\textit{Biometrics} 33(1): 159--174.

\bibitem[Matsuura et~al.(2022)]{Matsuura2022}
Matsuura, R., S. Suzuki, M. Saeki, T. Ogawa, and Y. Matsuyama. 2022.
Refinement of utterance fluency feature extraction and automated scoring of L2 oral fluency with dialogic features.
In \textit{Proceedings of 2022 APSIPA ASC}, 1312--1320.

\bibitem[Matsuura et~al.(2025)]{Matsuura2025}
Matsuura, R., S. Suzuki, K. Takizawa, M. Saeki, and Y. Matsuyama. 2025.
Gauging the validity of machine learning-based temporal feature annotation to measure fluency in speech automatically.
\textit{Research Methods in Applied Linguistics} 4: 100177.

\bibitem[Morin and Marttinen Larsson(2025)]{MorinLarsson2025}
Morin, C., and M. Marttinen Larsson. 2025.
Large corpora and large language models: A replicable method for automating grammatical annotation.
\textit{Linguistics Vanguard} 11(1): 501--510.

\bibitem[Plonsky and Oswald(2014)]{PlonskyOswald2014}
Plonsky, L., and F. L. Oswald. 2014.
How big is ``big''? Interpreting effect sizes in L2 research.
\textit{Language Learning} 64(4): 878--912.

\bibitem[Rose(2020)]{Rose2020}
Rose, R. L. 2020.
Fluidity: Real-time feedback on acoustic measures of second language speech fluency.
In \textit{Proceedings of the International Conference on Speech Prosody 2020}, 774--778.

\bibitem[Segalowitz(2010)]{Segalowitz2010}
Segalowitz, N. 2010.
\textit{Cognitive Bases of Second Language Fluency}.
Routledge.

\bibitem[Skidmore and Moore(2023)]{SkidmoreMoore2023}
Skidmore, L., and R. K. Moore. 2023.
BERT models for spoken learner English disfluency detection.
In \textit{Proceedings of SLaTE 2023}, 91--92.

\bibitem[Suzuki and Hanzawa(2022)]{SuzukiHanzawa2022}
Suzuki, Y., and K. Hanzawa. 2022.
Massed task repetition is a double-edged sword for fluency development: An EFL classroom study.
\textit{Studies in Second Language Acquisition} 44(2): 536--561.

\bibitem[Suzuki and Kormos(2023)]{SuzukiKormos2023}
Suzuki, S., and J. Kormos. 2023.
The multidimensionality of second language oral fluency: Interfacing cognitive fluency and utterance fluency.
\textit{Studies in Second Language Acquisition} 45(1): 38--64.

\bibitem[Suzuki and R\'{e}v\'{e}sz(2023)]{SuzukiRevesz2023}
Suzuki, S., and A. R\'{e}v\'{e}sz. 2023.
Measuring speaking and writing fluency: A methodological synthesis focusing on automaticity.
In Y. Suzuki (Ed.), \textit{Practice and Automatization in Second Language Research} (pp.\ 247--266). Routledge.

\bibitem[Suzuki et~al.(2021)]{SuzukiEtAl2021}
Suzuki, S., J. Kormos, and T. Uchihara. 2021.
The relationship between utterance and perceived fluency: A meta-analysis of correlational studies.
\textit{Modern Language Journal} 105(2): 435--463.

\bibitem[Tavakoli and Skehan(2005)]{TavakoliSkehan2005}
Tavakoli, P., and P. Skehan. 2005.
Strategic planning, task structure and performance testing.
In R. Ellis (Ed.), \textit{Planning and Task Performance in a Second Language} (pp.\ 239--273). John Benjamins.

\bibitem[Tavakoli et~al.(2020)]{TavakoliEtAl2020}
Tavakoli, P., F. Nakatsuhara, and A. Hunter. 2020.
Aspects of fluency across assessed levels of speaking proficiency.
\textit{Modern Language Journal} 104(1): 169--191.

\bibitem[Tavakoli et~al.(2023)]{TavakoliEtAl2023}
Tavakoli, P., G. Kendon, S. Mazhurnaya, and A. Ziomek. 2023.
Assessment of fluency in the Test of English for Educational Purposes.
\textit{Language Testing} 40(3): 607--629.

\bibitem[Vercellotti and Hall(2024)]{VercellottiHall2024}
Vercellotti, M. L., and S. Hall. 2024.
Coding all clauses in L2 data: A call for consistency.
\textit{Research Methods in Applied Linguistics} 3: 100132.

\bibitem[Zayats et~al.(2019)]{ZayatsEtAl2019}
Zayats, V., T. Tran, R. Wright, C. Mansfield, and M. Ostendorf. 2019.
Disfluencies and human speech transcription errors.
In \textit{Proceedings of Interspeech 2019}, 3088--3092.

\bibitem[Zhu et~al.(2022)]{ZhuEtAl2022}
Zhu, G., J.-P. Caceres, and J. Salamon. 2022.
Filler word detection and classification: A dataset and benchmark.
In \textit{Proceedings of ICASSP 2022}, 2202--2206.

\end{thebibliography}

\end{document}
