"""
TextGrid Clause Segmentation V3

Uses the EXACT same clause detection logic as v1/v2
(Vercellotti & Hall 2024 rules via spaCy NLP).

V3 CHANGE: Neural Disfluency Detection
- Replaces rule-based filler/repetition preprocessing with a trained
  xlm-roberta-base disfluency detection model
- The model labels each word as fluent (0) or disfluent (1)
- Disfluent words (fillers, repetitions, self-corrections, false starts)
  are removed before clause segmentation
- Handles more disfluency types than the old regex approach

Usage:
    python textgrid_caf_segmenter_v3.py -i textgrids/ -o clauses/ --sentences sentences/
    python textgrid_caf_segmenter_v3.py -i input.TextGrid -o output.TextGrid
"""

import os
import sys
import re
import spacy
import torch
from typing import List, Tuple, Set, Dict
from dataclasses import dataclass
from praatio import textgrid
from praatio.utilities.constants import Interval
from transformers import AutoTokenizer, AutoModelForTokenClassification


# ==============================================================================
# Sentence Loader - Reads pre-computed sentences from JSON
# ==============================================================================

class SentenceLoader:
    """
    Loads pre-computed sentences from JSON files.
    
    Sentences are generated by sentence_segmenter.py using wtpsplit.
    This allows running sentence segmentation separately to avoid
    dependency conflicts.
    """
    
    def __init__(self, sentences_dir: str = None):
        """
        Args:
            sentences_dir: Directory containing sentence JSON files.
                          If None, will use inline fallback.
        """
        self.sentences_dir = sentences_dir
        self._cache = {}
    
    def load_sentences(self, filename: str) -> List[str]:
        """
        Load sentences for a file from JSON.
        
        Args:
            filename: Base filename (e.g., "ALL_001_F_GER_ENG_ST1.TextGrid")
            
        Returns:
            List of sentence strings, or None if not found
        """
        if self.sentences_dir is None:
            return None
        
        json_name = filename.replace('.TextGrid', '.json')
        json_path = os.path.join(self.sentences_dir, json_name)
        
        if json_path in self._cache:
            return self._cache[json_path]
        
        if os.path.exists(json_path):
            import json
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            sentences = data.get('sentences', [])
            self._cache[json_path] = sentences
            return sentences
        
        return None
    
    def detect_sentences_fallback(self, text: str) -> List[str]:
        """
        Fallback: Use spaCy's sentence detection.
        Less accurate for unpunctuated text but works without external dependencies.
        """
        # Simple fallback - split on common sentence starters
        # This is a heuristic for when no pre-computed sentences exist
        return [text]  # Return as single sentence


# ==============================================================================
# DisfluencyDetector - Neural disfluency detection (xlm-roberta-base)
# ==============================================================================

class DisfluencyDetector:
    """
    Detects disfluent words using a fine-tuned xlm-roberta-base model.
    Labels each word as fluent (0) or disfluent (1).
    Replaces the old rule-based filler/repetition removal.
    """

    # Candidate default model paths (searched upward from script location)
    MODEL_PATH_CANDIDATES = (
        os.path.join("shared", "disfluency_detector", "model_v2", "final"),
        os.path.join("en", "disfluency_test", "l2_disfluency_detector", "model_v2", "final"),
        os.path.join("disfluency_test", "l2_disfluency_detector", "model_v2", "final"),
    )

    def __init__(self, model_path: str = None):
        if model_path is None:
            # Auto-discover model by walking upward from script location.
            script_dir = os.path.dirname(os.path.abspath(__file__))
            search_dir = script_dir
            found = None
            for _ in range(8):
                for rel in self.MODEL_PATH_CANDIDATES:
                    candidate = os.path.join(search_dir, rel)
                    if os.path.exists(candidate):
                        found = candidate
                        break
                if found is not None:
                    break
                parent = os.path.dirname(search_dir)
                if parent == search_dir:
                    break
                search_dir = parent

            if found is not None:
                model_path = found
            else:
                # Keep deterministic fallback for error reporting.
                project_root = os.path.dirname(script_dir)
                model_path = os.path.join(project_root, self.MODEL_PATH_CANDIDATES[0])

        if not os.path.exists(model_path):
            raise FileNotFoundError(
                f"Disfluency model not found at: {model_path}\n"
                f"Expected one of:\n"
                f"  - <repo_root>/shared/disfluency_detector/model_v2/final\n"
                f"  - <repo_root>/en/disfluency_test/l2_disfluency_detector/model_v2/final"
            )

        print(f"Loading disfluency model from: {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=2)
        self.model.eval()
        print("Disfluency model loaded.")

    def detect(self, words: List[str]) -> List[int]:
        """
        Predict disfluency labels for a list of words.

        Args:
            words: List of space-separated words (English: just .split())

        Returns:
            List of labels: 0 = fluent, 1 = disfluent
        """
        if not words:
            return []

        inputs = self.tokenizer(
            words, is_split_into_words=True,
            return_tensors="pt", truncation=True
        )
        with torch.no_grad():
            logits = self.model(**inputs).logits
        preds = torch.argmax(logits, dim=2)[0].tolist()
        word_ids = inputs.word_ids()

        # Majority vote per word
        word_preds: Dict[int, List[int]] = {}
        for idx, wid in enumerate(word_ids):
            if wid is not None:
                word_preds.setdefault(wid, []).append(preds[idx])

        labels = []
        for i in range(len(words)):
            votes = word_preds.get(i, [0])
            labels.append(1 if sum(votes) > len(votes) / 2 else 0)

        return labels

    def clean(self, words: List[str]) -> Tuple[List[str], List[int], Dict]:
        """
        Remove disfluent words from a word list.

        Returns:
            (clean_words, removed_indices, info_dict)
        """
        labels = self.detect(words)
        clean_words = []
        removed_indices = []
        removed_words = []
        for i, (word, label) in enumerate(zip(words, labels)):
            if label == 1:
                removed_indices.append(i)
                removed_words.append(word)
            else:
                clean_words.append(word)

        info = {
            'original_length': len(words),
            'processed_length': len(clean_words),
            'disfluencies_removed': len(removed_indices),
            'removed_words': removed_words,
        }
        return clean_words, removed_indices, info


# ==============================================================================
# ClauseSegmenter - Vercellotti & Hall (2024) with Strict Coordinated VP Rule
# ==============================================================================

class ClauseSegmenter:
    """
    Segments L2 speech into clauses following Vercellotti & Hall (2024).
    REVISED VERSION with proper handling of:
    - Coordinated VPs (conj) as separate clauses
    - Parataxis as separate clauses
    - Copula constructions (with and without copula)
    - Existential constructions (there's)
    """

    # Stance verbs that can be tagged as MINOR when they take a ccomp
    # Per Vercellotti: allows separate analysis of epistemic hedges
    MINOR_STANCE_LEMMAS = {'think', 'believe', 'guess', 'suppose', 'know', 'feel', 'mean'}

    def __init__(self, nlp, preprocess: bool = True, debug: bool = False,
                 disfluency_detector: DisfluencyDetector = None):
        self.nlp = nlp
        self.preprocess = preprocess
        self.debug = debug
        self.disfluency_detector = disfluency_detector
        self.fillers = {'uh', 'um', 'er', 'ah', 'uhm', 'mm', 'hmm', 'eh', '<unk>', 'unk', 'huh'}

    def has_complement_or_adjunct(self, verb_token) -> bool:
        for child in verb_token.children:
            dep = child.dep_
            if dep in {'obj', 'iobj', 'ccomp', 'xcomp', 'attr', 'oprd', 'dative', 'dobj'}:
                return True
            if dep.startswith('obl'):
                return True
            if dep in {'advmod', 'npadvmod', 'prep', 'pobj', 'agent'}:
                return True
        return False

    def is_verb_like(self, token) -> bool:
        # Include VB* tags, VERB/AUX pos, and special tags like EX_VBZ (existential "there's")
        return (token.tag_.startswith('VB') or
                token.pos_ in {'VERB', 'AUX'} or
                'VB' in token.tag_)  # Catches EX_VBZ, etc.

    def has_copula_child(self, token) -> bool:
        for child in token.children:
            if child.dep_ == 'cop':
                return True
            # HEURISTIC: Catch mis-parsed copulas (often labeled as nsubj in conversational/fluency data)
            # e.g. "i'm sorry" -> "i"(nsubj) "'m"(nsubj) "sorry"(ROOT)
            if child.dep_ == 'nsubj' and child.text.lower() in ["'m", "'re", "'s", "is", "are", "was", "were"]:
                return True
        return False

    def is_auxiliary(self, token) -> bool:
        return token.dep_ == 'aux' or token.tag_ in {'MD'}

    def preprocess_text(self, text: str, remove_fillers: bool = False,
                       handle_repetitions: bool = True) -> Tuple[str, Dict]:
        info = {
            'fillers_removed': 0,
            'repetitions_collapsed': 0,
            'disfluencies_removed': 0,
            'removed_words': [],
            'original_length': len(text.split())
        }

        # V3: Strip unintelligible markers before any processing
        UNK_TOKENS = {'<unk>', 'unk'}
        words = text.split()
        # Track original indices of surviving words
        surviving = [(w, i) for i, w in enumerate(words) if w.lower() not in UNK_TOKENS]
        unk_removed = len(words) - len(surviving)
        if unk_removed:
            info['removed_words'].extend(['<unk>'] * unk_removed)
            info['disfluencies_removed'] += unk_removed
        words = [w for w, _ in surviving]
        kept_indices = [i for _, i in surviving]
        text = ' '.join(words)

        # V3: Use neural disfluency model if available
        if self.disfluency_detector is not None:
            words = text.split()
            clean_words, removed_indices, det_info = self.disfluency_detector.clean(words)
            info['disfluencies_removed'] = det_info['disfluencies_removed']
            info['removed_words'] = det_info['removed_words']
            # Update kept_indices: remove entries at removed_indices
            removed_set = set(removed_indices)
            kept_indices = [kept_indices[i] for i in range(len(kept_indices)) if i not in removed_set]
            text = ' '.join(clean_words)
        else:
            # Fallback: simple filler removal only (no repetition handling)
            if remove_fillers:
                words = text.split()
                new_kept = []
                filtered = []
                for j, word in enumerate(words):
                    clean = word.lower().strip('.,!?;:-"\'')
                    if clean not in self.fillers:
                        filtered.append(word)
                        new_kept.append(kept_indices[j])
                    else:
                        info['fillers_removed'] += 1
                kept_indices = new_kept
                text = ' '.join(filtered)

        text = re.sub(r'\s+', ' ', text).strip()
        info['processed_length'] = len(text.split())
        info['kept_indices'] = kept_indices
        return text, info

    def compute_syntactic_depth(self, token) -> int:
        """Compute how deep this token is in the syntactic tree"""
        depth = 0
        current = token
        while current.head != current:
            depth += 1
            current = current.head
            if depth > 100:
                break
        return depth

    def get_clause_span_with_exclusions(self, verb_token, excluded_indices: Set[int], doc) -> Tuple[int, int, str, Set[int]]:
        """
        Collect clause span, excluding coordinated VP siblings (conj) and parataxis.
        Returns (start, end, text, collected_indices).
        
        NOTE: Only verb-like conj elements are skipped (they represent separate clauses).
        Coordinated nouns (e.g., "city and corner") are included in the clause span.
        """
        clause_tokens = set()

        def collect(token, depth=0):
            if depth > 20:
                return

            clause_tokens.add(token.i)
            for child in token.children:
                # Skip coordinated elements ONLY if they are verb-like (separate clauses)
                # OR if they have a copula child (predicate adjectives like "sure" in "i'm not sure")
                # Coordinated nouns/adjectives without copula should be included in the clause
                if child.dep_ == 'conj':
                    if self.is_verb_like(child) or self.has_copula_child(child):
                        continue  # Skip - this is a separate clause
                    # For non-verb/non-copula conj, include them in the clause span
                # Skip parataxis - they are separate clauses
                if child.dep_ == 'parataxis':
                    continue
                # Skip advcl/ccomp/acl - they are separate clauses per V&H
                # BUT only skip if they would actually be counted as separate clauses
                # (VBG/VBN advcl/acl without complement should be included in parent)
                if child.dep_ in {'advcl', 'ccomp', 'acl', 'acl:relcl'}:
                    # Check if this would be counted as separate clause
                    if child.dep_ in {'advcl', 'acl'} and child.tag_ in {'VBG', 'VBN'}:
                        # Finite advcl with subordinating conjunction (while, because, etc.)
                        # is ALWAYS a separate clause per Vercellotti
                        has_mark = any(c.dep_ == 'mark' for c in child.children)
                        if has_mark:
                            continue  # Finite adverbial clause - always separate
                        elif not self.has_complement_or_adjunct(child):
                            # NOT a separate clause - include in parent span
                            pass
                        else:
                            continue  # Has complement - separate clause
                    else:
                        continue  # Other deps are always separate clauses
                # Skip xcomp only if it's verbal (has its own clause structure)
                # Keep xcomp if it's adjectival/participial (e.g., "looks confused")
                if child.dep_ == 'xcomp':
                    # If xcomp is ADJ or has no verb children, include it
                    if child.pos_ not in {'VERB', 'AUX'}:
                        pass  # Include adjectival xcomp
                    else:
                        continue  # Skip verbal xcomp
                # Skip tokens already claimed by deeper clauses
                if child.i in excluded_indices:
                    continue
                collect(child, depth + 1)

        collect(verb_token)

        if not clause_tokens:
            return (verb_token.i, verb_token.i + 1, verb_token.text, {verb_token.i})

        indices = sorted(clause_tokens)

        # Find the CONTIGUOUS segment containing the verb head
        # Only include tokens that are contiguous with the verb - don't skip over gaps
        verb_idx = verb_token.i

        # Find contiguous range around the verb
        contiguous_start = verb_idx
        contiguous_end = verb_idx

        # Expand backwards from verb
        while contiguous_start - 1 in clause_tokens:
            contiguous_start -= 1

        # Expand forwards from verb
        while contiguous_end + 1 in clause_tokens:
            contiguous_end += 1

        # Only keep tokens in the contiguous range around the verb
        contiguous_indices = [i for i in indices if contiguous_start <= i <= contiguous_end]
        
        # ALSO look for cc/mark tokens that precede the clause
        # This handles cases like:
        # - "and since their cases..." where "and" is a cc attached to parent
        # - "because for the man... he found" where "because" is a mark attached to verb
        #   but separated by a parataxis/disfluency
        min_clause_idx = min(contiguous_indices) if contiguous_indices else verb_idx
        clause_parent = verb_token.head  # The parent of this clause head
        
        # First, check if there's a mark/advmod/cc token in clause_tokens that got dropped
        # (mark tokens like "because", "since", "when" are children of verb_token)
        # (advmod tokens like "suddenly", "then" are children of verb_token)
        leading_deps = {'mark', 'advmod', 'cc'}
        
        # Iterate BACKWARDS from contiguous_start to support dependency chains
        # e.g. "as far as showing" -> "as" (head=far), "far" (head=showing)
        # By going backwards: 
        # 1. Process "far": head is showing (in contiguous). Match! Add "far".
        # 2. Process "as": head is "far" (now in contiguous). Match! Add "as".
        
        # Get indices preceding contiguous range - only those CLOSE to the clause
        # Limit to 10 tokens before to avoid grabbing tokens from unrelated clauses
        preceding_indices = sorted([i for i in indices if contiguous_start - 10 <= i < contiguous_start], reverse=True)
        
        for idx in preceding_indices:
            token = doc[idx]
            # Include mark/advmod/cc tokens ONLY if attached directly to verb or contiguous block
            # Do NOT include if attached to excluded_indices - that belongs to another clause
            if token.dep_ in leading_deps:
                if token.head.i == verb_idx or token.head.i in contiguous_indices:
                    contiguous_indices.append(idx)
        
        # Also look for cc tokens OUTSIDE clause_tokens
        min_clause_idx = min(contiguous_indices) if contiguous_indices else verb_idx
        for idx in range(min_clause_idx - 1, max(min_clause_idx - 5, -1), -1):  # Look back up to 5 tokens
            if idx in excluded_indices:
                break  # Don't cross into another clause's territory
            token = doc[idx]
            # Include cc that:
            # 1. Is attached to the same parent as this clause, OR
            # 2. Is attached to this clause's head (verb_token)
            if token.dep_ == 'cc' and (token.head == clause_parent or token.head == verb_token):
                contiguous_indices.append(idx)
                break  # Only take the immediately preceding cc
        
        contiguous_indices = sorted(set(contiguous_indices))

        start, end = contiguous_indices[0], contiguous_indices[-1] + 1
        text = ' '.join(doc[i].text for i in contiguous_indices)
        return (start, end, text, set(contiguous_indices))

    def find_all_clauses(self, doc) -> List[Tuple[int, int, str, str]]:
        """Find all clauses following Vercellotti & Hall (2024)."""
        clause_heads = []

        for token in doc:
            clause_type = None

            if self.debug:
                if self.is_verb_like(token) or (token.pos_ in {'ADJ', 'NOUN'} or token.tag_ in {'JJ', 'NN'}):
                    has_comp = self.has_complement_or_adjunct(token) if self.is_verb_like(token) else False
                    has_cop = self.has_copula_child(token)


            # 1. ROOT (independent clause)
            if token.dep_ == 'ROOT':
                if self.is_verb_like(token):
                    # Check for MINOR stance verbs (e.g., "I think" with ccomp)
                    if token.lemma_.lower() in self.MINOR_STANCE_LEMMAS:
                        has_ccomp = any(ch.dep_ == 'ccomp' for ch in token.children)
                        if has_ccomp:
                            clause_type = 'minor'
                        else:
                            clause_type = 'independent'
                    else:
                        clause_type = 'independent'
                elif token.pos_ in {'ADJ', 'NOUN'} or token.tag_ in {'JJ', 'NN', 'JJR', 'JJS', 'NNS', 'NNP', 'NNPS'}:
                    if self.has_copula_child(token):
                        clause_type = 'independent'
                    else:
                        clause_type = 'independent_copula_less'

            # 2. Subordinate clauses
            elif token.dep_ in {'advcl', 'relcl', 'acl:relcl', 'ccomp', 'acl', 'csubj'}:
                if self.is_verb_like(token):
                    if token.tag_ in {'VBG', 'VBN'} and token.dep_ in {'advcl', 'acl'}:
                        # Finite advcl with subordinating conjunction (while, because, etc.)
                        # is ALWAYS a separate clause per Vercellotti
                        has_mark = any(c.dep_ == 'mark' for c in token.children)
                        if has_mark:
                            clause_type = f'subordinate_{token.dep_}'
                        elif self.has_complement_or_adjunct(token):
                            clause_type = f'subordinate_{token.dep_}'
                        else:
                            if self.debug:
                                print(f"  → SKIPPED (nonfinite {token.dep_} without complement)")
                    else:
                        clause_type = f'subordinate_{token.dep_}'
                elif token.pos_ in {'ADJ', 'NOUN'} or token.tag_ in {'JJ', 'NN', 'JJR', 'JJS', 'NNS', 'NNP', 'NNPS'}:
                    if self.has_copula_child(token):
                        clause_type = f'subordinate_{token.dep_}'
                        if self.debug:
                            print(f"  → COUNTED as subordinate copula construction")
                    elif token.dep_ in {'ccomp', 'csubj'}:
                        clause_type = f'subordinate_{token.dep_}_copula_less'
                        if self.debug:
                            print(f"  → COUNTED as subordinate copula-less construction")

            # 3. Coordinated VPs
            elif token.dep_ == 'conj' and self.is_verb_like(token):
                head = token.head
                if head:
                    head_is_clause = (
                        self.is_verb_like(head) or
                        (head.pos_ in {'ADJ', 'NOUN'} and self.has_copula_child(head)) or
                        head.tag_ in {'JJ', 'NN', 'JJR', 'JJS', 'NNS', 'NNP', 'NNPS'}
                    )
                    if head_is_clause:
                        # V&H rule: coordinated VP is a clause ONLY if it has its OWN complement/adjunct
                        # Strict Vercellotti: bare coordinated verbs without own element are NOT clauses
                        if self.has_complement_or_adjunct(token):
                            clause_type = 'coordinated_vp'

            # 4. Nonfinite xcomp
            elif token.dep_ == 'xcomp' and self.is_verb_like(token):
                if self.has_complement_or_adjunct(token):
                    clause_type = 'nonfinite_xcomp'

            # 5. Participles with complements
            elif self.is_verb_like(token) and token.tag_ in {'VBG', 'VBN'}:
                if not self.is_auxiliary(token):
                    # Skip progressive/passive forms (have aux child like "is working", "was taken")
                    has_aux_child = any(child.dep_ == 'aux' for child in token.children)
                    if not has_aux_child:
                        if self.has_complement_or_adjunct(token):
                            if token.dep_ not in {'amod', 'compound', 'advcl', 'acl'}:
                                clause_type = 'nonfinite_participle'

            # 6. Parataxis (including copula constructions)
            elif token.dep_ == 'parataxis':
                if self.is_verb_like(token):
                    clause_type = 'parataxis'
                elif (token.pos_ in {'ADJ', 'NOUN'} or token.tag_ in {'JJ', 'NN', 'JJR', 'JJS', 'NNS', 'NNP', 'NNPS'}):
                    if self.has_copula_child(token):
                        clause_type = 'parataxis'

            if clause_type:
                clause_heads.append((token.i, clause_type, token))
                if self.debug:
                    print(f"  → COUNTED as {clause_type}")

        # TWO-PASS: Sort by depth (deepest first)
        clause_heads_with_depth = []
        for head_idx, clause_type, head_token in clause_heads:
            depth = self.compute_syntactic_depth(head_token)
            clause_heads_with_depth.append((depth, head_idx, clause_type, head_token))

        clause_heads_with_depth.sort(key=lambda x: (-x[0], x[1]))

        excluded_indices = set()
        result = []

        for depth, head_idx, clause_type, head_token in clause_heads_with_depth:
            start, end, text, collected = self.get_clause_span_with_exclusions(head_token, excluded_indices, doc)
            excluded_indices.update(collected)

            # Validate clause using linguistic criteria
            words = text.split()
            is_valid_clause = False
            
            if len(words) >= 3:
                is_valid_clause = True
            elif len(words) == 2:
                # 2-word clauses need validation: must have subject or object in span
                # "was preparing" without subject is invalid
                # "i think" with subject is valid
                core_deps = {'nsubj', 'nsubj:pass', 'obj', 'iobj', 'ccomp', 'xcomp', 
                             'attr', 'oprd', 'expl', 'csubj', 'dobj'}
                # Clause deps that are counted separately but still validate parent
                clause_deps = {'ccomp', 'xcomp', 'csubj', 'advcl', 'acl', 'acl:relcl'}
                has_core_in_span = any(
                    child.dep_ in core_deps and child.i in collected
                    for child in head_token.children
                )
                # Also valid if head has a clausal complement (counted as separate clause)
                has_clause_child = any(
                    child.dep_ in clause_deps
                    for child in head_token.children
                )
                # Also check if the head token itself is a subject/object of something in span
                if has_core_in_span or has_clause_child:
                    is_valid_clause = True
                elif head_token.dep_ in core_deps:
                    # Head is a dependent of something - check if parent is in span
                    is_valid_clause = head_token.head.i in collected
            elif len(words) == 1:
                # Single-word clause validation:
                # 1. Contractions (starting with apostrophe) cannot stand alone as clauses
                if head_token.text.startswith("'"):
                    is_valid_clause = False
                # 2. Check if head token has required clause structure (subject or complement)
                #    The dependent must be IN THE COLLECTED SPAN (not claimed by another clause)
                elif self.is_verb_like(head_token) or self.has_copula_child(head_token):
                    # A valid single-word clause head must have at least one core dependent
                    # that is actually included in this clause's span
                    core_deps = {'nsubj', 'nsubj:pass', 'obj', 'iobj', 'ccomp', 'xcomp', 
                                 'attr', 'oprd', 'expl', 'csubj', 'dobj'}
                    has_core_dependent_in_span = any(
                        child.dep_ in core_deps and child.i in collected
                        for child in head_token.children
                    )
                    is_valid_clause = has_core_dependent_in_span
            
            if is_valid_clause:
                # Return actual collected indices for proper time alignment
                result.append((start, end, clause_type, text, sorted(collected)))

        # === Rule 8: Discontinuous main clause (relcl interruption) ===
        # When a relcl separates a verb from its subject, the verb's clause
        # is missing its subject.  If that clause also has a ccomp child that
        # is a separate result clause, merge the ccomp INTO the verb's clause
        # to avoid a short fragment + isolated ccomp.
        ccomp_merge_map = {}  # ccomp_result_idx -> parent_result_idx
        for i, (s, e, ct, txt, idx) in enumerate(result):
            idx_set = set(idx)
            # Find the clause head verb
            head_token = None
            for tidx in idx:
                tok = doc[tidx]
                if self.is_verb_like(tok) and tok.dep_ in ('ROOT', 'conj', 'parataxis', 'advcl', 'ccomp'):
                    head_token = tok
                    break
            if head_token is None:
                continue
            # Check: subject exists but is NOT in this clause's span
            nsubj_children = [ch for ch in head_token.children
                              if ch.dep_ in ('nsubj', 'nsubj:pass', 'expl')]
            if not nsubj_children:
                continue
            nsubj_in_span = any(ch.i in idx_set for ch in nsubj_children)
            if nsubj_in_span:
                continue  # Subject is present — no interruption
            # Confirm a relcl on the subject caused the gap
            has_relcl = any(
                gc.dep_ in ('relcl', 'acl:relcl', 'acl')
                for nsubj in nsubj_children
                for gc in nsubj.children
            )
            if not has_relcl:
                continue
            # Only merge if parent clause is SHORT (≤ 3 non-filler words) —
            # i.e. just verb + connectors, not a clause with its own object.
            non_filler_words = [w for w in txt.split()
                                if w.lower() not in self.fillers]
            if len(non_filler_words) > 3:
                continue
            # Find ccomp child that is a separate clause in results
            for child in head_token.children:
                if child.dep_ != 'ccomp':
                    continue
                for j, (s2, e2, ct2, txt2, idx2) in enumerate(result):
                    if j != i and ct2.startswith('subordinate_ccomp') and child.i in idx2:
                        ccomp_merge_map[j] = i
                        break

        # Apply merges: append ccomp text to parent clause
        merged_out = set()
        for ccomp_idx, parent_idx in ccomp_merge_map.items():
            if ccomp_idx in merged_out:
                continue
            ps, pe, pct, ptxt, pidx = result[parent_idx]
            cs, ce, cct, ctxt, cidx = result[ccomp_idx]
            new_indices = sorted(set(list(pidx) + list(cidx)))
            new_text = ptxt + ' ' + ctxt
            result[parent_idx] = (
                min(ps, cs), max(pe, ce), pct, new_text, new_indices
            )
            merged_out.add(ccomp_idx)

        if merged_out:
            result = [r for i, r in enumerate(result) if i not in merged_out]

        return sorted(result, key=lambda x: x[0])

    def segment(self, text: str, verbose: bool = True,
                remove_fillers: bool = False,
                handle_repetitions: bool = True) -> List[Tuple[int, int, str, str, List[int]]]:
        """Segment text into clauses.
        
        Returns list of tuples: (start_idx, end_idx, clause_type, clause_text, token_indices)
        where token_indices are the ACTUAL collected token indices (not contiguous range).
        """
        if self.preprocess:
            text, info = self.preprocess_text(text, remove_fillers, handle_repetitions)
            disfluencies = info.get('disfluencies_removed', 0)
            fillers = info.get('fillers_removed', 0)
            if verbose and (disfluencies > 0 or fillers > 0):
                print(f"Preprocessing (disfluency model):")
                print(f"  - Disfluencies removed: {disfluencies}")
                if info.get('removed_words'):
                    preview = info['removed_words'][:10]
                    print(f"  - Removed: {', '.join(preview)}{'...' if len(info['removed_words']) > 10 else ''}")
                print(f"  - Words: {info['original_length']} -> {info['processed_length']}")
                print()

        doc = self.nlp(text)
        clauses = self.find_all_clauses(doc)

        if verbose:
            print(f"{'TYPE':<30} | {'CLAUSE TEXT'}")
            print("-" * 110)
            for start, end, ctype, ctext, indices in clauses:
                clean = ' '.join(ctext.split())
                if len(clean) > 75:
                    clean = clean[:72] + "..."
                print(f"{ctype:<30} | {clean}")
            print(f"\nTotal clauses: {len(clauses)}")

        return clauses

    def calculate_complexity(self, clauses: List[Tuple[int, int, str, str, List[int]]],
                           remove_fillers: bool = True) -> Dict:
        total = len(clauses)
        independent = sum(1 for _, _, t, _, _ in clauses if t.startswith('independent'))
        subordinate = sum(1 for _, _, t, _, _ in clauses if t.startswith('subordinate'))
        coordinated = sum(1 for _, _, t, _, _ in clauses if t == 'coordinated_vp')
        nonfinite = sum(1 for _, _, t, _, _ in clauses if t.startswith('nonfinite'))
        parataxis = sum(1 for _, _, t, _, _ in clauses if t == 'parataxis')
        subordination_ratio = (subordinate + nonfinite) / total if total > 0 else 0
        lengths = []
        for _, _, _, text, _ in clauses:
            words = text.split()
            if remove_fillers:
                words = [w for w in words if w.lower() not in {'uh', 'um', '<unk>', 'unk'}]
            lengths.append(len(words))
        avg_length = sum(lengths) / len(lengths) if lengths else 0
        return {
            'total_clauses': total,
            'independent': independent,
            'subordinate': subordinate,
            'coordinated_vp': coordinated,
            'nonfinite': nonfinite,
            'parataxis': parataxis,
            'subordination_ratio': round(subordination_ratio, 3),
            'avg_clause_length': round(avg_length, 2)
        }

    def export_to_dataframe(self, clauses: List[Tuple[int, int, str, str, List[int]]], text_id: str = None):
        data = []
        for i, (start, end, ctype, ctext, indices) in enumerate(clauses, 1):
            data.append({
                'text_id': text_id,
                'clause_num': i,
                'start_idx': start,
                'end_idx': end,
                'clause_type': ctype,
                'clause_text': ctext.strip(),
                'clause_length': len(ctext.split())
            })
        return pd.DataFrame(data)


# ==============================================================================
# TextGrid Handler
# ==============================================================================

@dataclass
class WordInterval:
    start: float
    end: float
    text: str
    is_pause: bool = False
    syllable_count: int = 0

@dataclass
class PauseInterval:
    start: float
    end: float
    duration: float
    location: str = "unknown"


class TextGridHandler:

    PAUSE_MARKERS = {'', 'sp', 'sil', '<sil>', 'pause', '#', '...',
                     '<p>', '<pause>', 'breath', '<breath>'}

    VOWELS = {'AA', 'AE', 'AH', 'AO', 'AW', 'AX', 'AXR', 'AY',
              'EH', 'ER', 'EY', 'IH', 'IX', 'IY', 'OW', 'OY',
              'UH', 'UW', 'UX'}

    def __init__(self, filepath: str):
        self.filepath = filepath
        self.tg = textgrid.openTextgrid(filepath, includeEmptyIntervals=True)
        self.words: List[WordInterval] = []
        self.phones: List[Tuple[float, float, str]] = []
        self.total_syllables = 0
        self._extract_tiers()

    def _extract_tiers(self):
        tier_names = self.tg.tierNames

        word_tier_name = None
        for name in ['words', 'word', 'Word', 'Words', 'utt - words']:
            if name in tier_names:
                word_tier_name = name
                break
        if not word_tier_name:
            word_tier_name = tier_names[0]

        phone_tier_name = None
        for name in ['phones', 'phone', 'Phone', 'Phones']:
            if name in tier_names:
                phone_tier_name = name
                break

        if phone_tier_name:
            phone_tier = self.tg.getTier(phone_tier_name)
            for entry in phone_tier.entries:
                self.phones.append((entry.start, entry.end, entry.label.strip()))

        word_tier = self.tg.getTier(word_tier_name)
        for entry in word_tier.entries:
            text = entry.label.strip()
            is_pause = text.lower() in self.PAUSE_MARKERS or text == ''

            syllables = 0
            if not is_pause and self.phones:
                syllables = self._count_syllables_in_range(entry.start, entry.end)

            self.words.append(WordInterval(
                start=entry.start,
                end=entry.end,
                text=text,
                is_pause=is_pause,
                syllable_count=syllables
            ))

            if not is_pause:
                self.total_syllables += syllables

    def _count_syllables_in_range(self, start: float, end: float) -> int:
        count = 0
        for p_start, p_end, label in self.phones:
            if p_start >= start and p_end <= end:
                base_phone = label.split(',')[0].strip()
                base_phone = re.sub(r'[0-9*]', '', base_phone).upper()
                if base_phone in self.VOWELS:
                    count += 1
        return max(count, 1) if count == 0 else count

    def get_transcript(self) -> str:
        return ' '.join(w.text for w in self.words if not w.is_pause and w.text)

    def get_word_list(self) -> List[WordInterval]:
        return [w for w in self.words if not w.is_pause and w.text]

    def get_pauses(self, min_duration: float = 0.25) -> List[PauseInterval]:
        pauses = []
        for w in self.words:
            if w.is_pause:
                duration = w.end - w.start
                if duration >= min_duration:
                    pauses.append(PauseInterval(
                        start=w.start,
                        end=w.end,
                        duration=duration
                    ))
        return pauses

    def _strip_boundary_fillers_with_timing(self, start: float, end: float, text: str, word_indices: list = None):
        """Strip fillers from start and end of text, and adjust timing accordingly.
        Uses the provided text (not reconstructed from time range) and adjusts timing.
        Returns (new_start, new_end, stripped_text)"""
        fillers = {'uh', 'um', 'er', 'ah', 'uhm', 'mm', 'hmm', 'eh', '<unk>', 'unk'}
        
        # Strip ALL fillers from clause text (boundary and interior)
        text_words = [w for w in text.split() if w.lower() not in fillers]
        
        if not text_words:
            return start, end, text
        
        stripped_text = ' '.join(text_words)
        first_word = text_words[0].lower()
        last_word = text_words[-1].lower()
        
        # Find timing by matching first/last words in time range
        words_in_interval = []
        for w in self.words:
            if w.is_pause or not w.text:
                continue
            w_center = (w.start + w.end) / 2
            if start <= w_center < end:
                words_in_interval.append(w)
        
        if not words_in_interval:
            return start, end, stripped_text
        
        # Find first matching word for start time
        new_start = start
        for w in words_in_interval:
            if w.text.lower() == first_word:
                new_start = w.start
                break
        
        # Find last matching word for end time
        new_end = end
        for w in reversed(words_in_interval):
            if w.text.lower() == last_word:
                new_end = w.end
                break
        
        # Fallback: strip fillers from words_in_interval for timing
        while words_in_interval and words_in_interval[0].text.lower() in fillers:
            words_in_interval.pop(0)
        while words_in_interval and words_in_interval[-1].text.lower() in fillers:
            words_in_interval.pop()
        
        if not words_in_interval:
            return start, end, stripped_text
        
        # Use fallback timing from non-filler words if word matching failed
        if new_start == start:
            new_start = words_in_interval[0].start
        if new_end == end:
            new_end = words_in_interval[-1].end
        
        return new_start, new_end, stripped_text

    def add_clause_tier(self, clauses: List[Dict], tier_name: str = "clauses"):
        """Add a single clause tier with all clauses as non-overlapping intervals.
        Simple approach: sort by start time, trim overlaps at boundaries."""
        from praatio.data_classes.interval_tier import IntervalTier

        min_time = self.tg.minTimestamp
        max_time = self.tg.maxTimestamp

        if not clauses:
            intervals = [Interval(min_time, max_time, "")]
            new_tier = IntervalTier(tier_name, intervals, min_time, max_time)
            self.tg.addTier(new_tier)
            return

        # Sort by start_time to process in chronological order
        sorted_clauses = sorted(clauses, key=lambda c: (c['start_time'], c['end_time']))

        # Build non-overlapping intervals - simple approach
        final_clauses = []
        last_end = min_time

        for clause in sorted_clauses:
            start = max(clause['start_time'], last_end)  # Don't overlap with previous
            end = clause['end_time']

            if end > start + 0.01:  # Valid interval
                final_clauses.append({
                    'start': start,
                    'end': end,
                    'label': clause['text'],
                    'type': clause['type'],
                    'word_indices': clause.get('word_indices', None)
                })
                last_end = end

        # Sort by start time for TextGrid interval building
        final_clauses.sort(key=lambda x: x['start'])

        # Build TextGrid intervals with gaps
        intervals = []
        created_intervals = [] # To return for logging
        current_time = min_time

        for clause in final_clauses:
            orig_start, orig_end = clause['start'], clause['end']
            orig_label = clause.get('label', '')
            word_indices = clause.get('word_indices', None)
            
            # Strip boundary fillers and adjust timing - use specific word indices if available
            start, end, label = self._strip_boundary_fillers_with_timing(orig_start, orig_end, orig_label, word_indices)
            
            if start > current_time + 0.001:
                # Add empty gap for filler region
                intervals.append(Interval(current_time, start, ""))
            
            intervals.append(Interval(start, end, label))
            if label.strip():
                created_intervals.append({
                    'label': label,
                    'type': clause['type'],
                    'start': start,
                    'end': end
                })
            current_time = end

        if current_time < max_time - 0.001:
            intervals.append(Interval(current_time, max_time, ""))

        if not intervals:
            intervals = [Interval(min_time, max_time, "")]

        if tier_name in self.tg.tierNames:
            self.tg.removeTier(tier_name)

        new_tier = IntervalTier(tier_name, intervals, min_time, max_time)
        self.tg.addTier(new_tier)
        
        return created_intervals

    def _get_words_in_interval(self, start: float, end: float) -> str:
        """Get the text of all words that fall within the given interval"""
        words = []
        for w in self.words:
            if w.is_pause or not w.text:
                continue
            # Check if word is substantially inside the interval (using center point)
            # Use exclusive end to avoid including words at exact boundary
            w_center = (w.start + w.end) / 2
            if start <= w_center < end:
                words.append(w.text)
        return " ".join(words)

    def add_disfluency_tier(self, disfluency_labels: List[int], tier_name: str = "disfluency"):
        """Add a tier marking disfluent words detected by the neural model.
        
        Args:
            disfluency_labels: List of 0/1 labels aligned to get_word_list() indices.
                              1 = disfluent, 0 = fluent.
            tier_name: Name of the new tier.
        """
        from praatio.data_classes.interval_tier import IntervalTier

        min_time = self.tg.minTimestamp
        max_time = self.tg.maxTimestamp

        word_list = self.get_word_list()
        if len(disfluency_labels) != len(word_list):
            print(f"  WARNING: disfluency labels ({len(disfluency_labels)}) != words ({len(word_list)}), skipping tier")
            return

        # Build intervals: disfluent words get their text as label, fluent words are empty
        intervals = []
        current_time = min_time

        for w, label in zip(word_list, disfluency_labels):
            # Fill gap before this word (pauses, etc.)
            if w.start > current_time + 0.001:
                intervals.append(Interval(current_time, w.start, ""))

            if label == 1:
                intervals.append(Interval(w.start, w.end, w.text))
            else:
                intervals.append(Interval(w.start, w.end, ""))

            current_time = w.end

        # Fill remaining time
        if current_time < max_time - 0.001:
            intervals.append(Interval(current_time, max_time, ""))

        if not intervals:
            intervals = [Interval(min_time, max_time, "")]

        if tier_name in self.tg.tierNames:
            self.tg.removeTier(tier_name)

        new_tier = IntervalTier(tier_name, intervals, min_time, max_time)
        self.tg.addTier(new_tier)

        disfluent_count = sum(disfluency_labels)
        print(f"  Disfluency tier: {disfluent_count} disfluent words marked")

    def save(self, output_path: str):
        self.tg.save(output_path, format="short_textgrid", includeBlankSpaces=True)


# ==============================================================================
# Clause Aligner
# ==============================================================================

class ClauseAligner:
    """Align clause boundaries to TextGrid word times"""

    def __init__(self, tg_handler: TextGridHandler, segmenter: ClauseSegmenter):
        self.tg = tg_handler
        self.segmenter = segmenter

    def align_clauses(self, remove_fillers: bool = True,
                     handle_repetitions: bool = True,
                     disfluency_labels: List[int] = None) -> List[Dict]:
        """Align clause boundaries to actual word times.
        
        Uses actual collected token indices (not contiguous range) for proper alignment.
        Handles gaps in token indices to ensure correct word matching.
        
        Args:
            disfluency_labels: Optional list of 0/1 labels aligned to tg_words.
                              1 = disfluent. Used to exclude disfluent words from clause text.
        """
        tg_words = self.tg.get_word_list()
        transcript = self.tg.get_transcript()

        clauses = self.segmenter.segment(
            transcript,
            verbose=False,
            remove_fillers=remove_fillers,
            handle_repetitions=handle_repetitions
        )

        if not clauses:
            return []

        # === Resolve Nesting / Crossing ===
        # Detect cases where a clause starts with a connector (e.g. "But", "And")
        # but is immediately interrupted by a subordinate clause.
        # Move the connector to the subordinate clause to avoid nesting/overlap.
        
        # Sort by start token index (use min() because indices might be unsorted)
        clauses.sort(key=lambda x: min(x[4]) if x[4] else 0)
        
        # We need to allow modification, so convert to lists
        mutable_clauses = []
        for c in clauses:
            c_list = list(c)
            c_list[4] = sorted(c_list[4]) if c_list[4] else []
            mutable_clauses.append(c_list)
            
        # === Gap Claiming (Recover Dropped Connectors) ===
        # Detect numeric gaps between clauses. If the gap corresponds to valid tokens (words)
        # that are likely connectors/adverbs (e.g. "suddenly"), claim them for the NEXT clause.
        i = 0
        while i < len(mutable_clauses) - 1:
            curr = mutable_clauses[i]
            next_c = mutable_clauses[i+1]
            
            c_indices = curr[4]
            n_indices = next_c[4]
            
            if not c_indices or not n_indices:
                i += 1
                continue
                
            c_end_idx = max(c_indices)
            n_start_idx = min(n_indices)
            
            # Check for Gap
            if n_start_idx > c_end_idx + 1:
                gap_start = c_end_idx + 1
                gap_end = n_start_idx  # Exclusive
                gap_indices = list(range(gap_start, gap_end))
                
                # Check gap size (small gaps only)
                if len(gap_indices) <= 3:
                # Check content of gap
                     # We need text from tg.words (filtered to match transcript indices)
                     word_list = self.tg.get_word_list()
                     gap_words = []
                     
                     for g_idx in gap_indices:
                         if g_idx < len(word_list):
                             w_text = word_list[g_idx].text.strip()
                             gap_words.append(w_text)
                     
                     if not gap_words:
                         i += 1
                         continue

                     # Dynamic POS Check using Spacy
                     # "no this kind of shit regrex hard solution"
                     gap_text = " ".join(gap_words)
                     doc = self.segmenter.nlp(gap_text)
                     
                     # Check if gap consists mainly of connectors, adverbs, or fillers
                     # We claim it if the FIRST word is a connector/adverb, or if the whole thing is junk.
                     # Actually, if the gap is "suddenly", it's ADV.
                     # If "and uh", it's CCONJ + INTJ.
                     
                     first_token_pos = doc[0].pos_ if len(doc) > 0 else ""
                     is_claimable = first_token_pos in ['CCONJ', 'SCONJ', 'ADV', 'INTJ', 'PART']
                     
                     if is_claimable:
                         # Claim logic: Add indices to NEXT clause
                         next_c[4] = sorted(gap_indices + next_c[4])
                         # Update text
                         next_c[3] = gap_text + " " + next_c[3]
                         # print(f"DEBUG: Claimed gap '{gap_text}' into '{next_c[3]}'")
                         
            i += 1
            
        # === Minimal Fragment Merger ===
        # Only merge clauses that are explicitly marked as 'fragment' type
        # Do NOT merge based on content analysis - preserve clause boundaries
        i = 0
        while i < len(mutable_clauses):
            curr = mutable_clauses[i]
            c_type = curr[2]
            
            # ONLY merge if explicitly a fragment type
            if c_type == 'fragment':
                if i < len(mutable_clauses) - 1:
                    # Merge Forward
                    next_c = mutable_clauses[i+1]
                    next_c[4] = sorted(list(set(curr[4] + next_c[4])))
                    next_c[3] = curr[3] + " " + next_c[3]
                    mutable_clauses.pop(i)
                    continue
                elif i > 0:
                    # Merge Backward
                    prev = mutable_clauses[i-1]
                    prev[4] = sorted(list(set(prev[4] + curr[4])))
                    prev[3] = prev[3] + " " + curr[3]
                    mutable_clauses.pop(i)
                    continue
            i += 1
        
        # Simple heuristic: Look for Clause A wrapping Clause B
        # A: indices [0, 8, 9] (0 is "But")
        # B: indices [1, 2, 3...] ("when...")
        
        # Iterate through clauses to find wrappings
        # Note: We restart search after modification to be safe, or just single pass?
        # Single pass with lookahead should suffice for this corpus complexity.
        
        for i in range(len(mutable_clauses)):
            parent = mutable_clauses[i]
            p_indices = parent[4]
            if len(p_indices) < 2:
                continue
                
            # Check if parent has a gap at the beginning
            # e.g. indices 0, 8... gap between 0 and 8.
            if len(p_indices) > 1 and p_indices[1] - p_indices[0] > 1:
                gap_start = p_indices[0] + 1
                gap_end = p_indices[1]
                
                # Check if next clause (i+1) fits entirely in this gap
                if i + 1 < len(mutable_clauses):
                    child = mutable_clauses[i+1]
                    c_indices = child[4]
                    if not c_indices:
                        continue
                        
                    c_min = min(c_indices)
                    c_max = max(c_indices)
                    
                    # Strictly inside the gap
                    if c_min >= gap_start and c_max < gap_end:
                         # Found nesting! "But [when...] he..."
                         # Move p_indices[0] (the connector) to child
                         p_text = parent[3]
                         c_text = child[3]
                         connector_idx = p_indices[0]
                         
                         parent[4] = p_indices[1:] # Remove first token
                         child[4] = sorted([connector_idx] + c_indices) # Add to child
                         
                         p_words = p_text.split()
                         if len(p_words) > 0:
                             connector_word = p_words[0]
                             parent[3] = " ".join(p_words[1:])
                             child[3] = connector_word + " " + c_text
        
        clauses = [tuple(c) for c in mutable_clauses]
        # ==================================

        processed_text, preproc_info = self.segmenter.preprocess_text(
            transcript, remove_fillers, handle_repetitions
        )
        processed_doc = self.segmenter.nlp(processed_text)
        original_words = [w.text.lower() for w in tg_words]
        
        # Build spaCy token → original word mapping via kept_indices + char offsets.
        # kept_indices[j] = original word index for processed word j.
        # spaCy may split words (contractions: "don't" → "do","n't"), so
        # token index != word index. Map via character offsets.
        kept_indices = preproc_info.get('kept_indices', None)
        token_to_orig = {}  # spaCy token index → original word index
        if kept_indices is not None:
            import bisect
            proc_words = processed_text.split()
            # Build char-offset start for each processed word
            word_char_starts = []
            pos = 0
            for pw in proc_words:
                word_char_starts.append(pos)
                pos += len(pw) + 1  # +1 for space
            for tok in processed_doc:
                widx = bisect.bisect_right(word_char_starts, tok.idx) - 1
                widx = max(0, min(widx, len(kept_indices) - 1))
                token_to_orig[tok.i] = kept_indices[widx]

        # Align all clauses to word indices using ACTUAL collected token indices
        # Sort by minimum token index to process in textual order
        # Sort clauses by start index, then by length (descending) to prioritize longer clauses
        clauses_sorted = sorted(clauses, key=lambda c: (min(c[4]) if c[4] else 0, -len(c[4])))
        
        aligned_clauses = []

        claimed_word_indices = set()  # Track indices already claimed by clauses


        for start_idx, end_idx, clause_type, clause_text, token_indices in clauses_sorted:
            # Get token texts with their indices to preserve gap information
            clause_token_texts = [processed_doc[i].text.lower() for i in token_indices]

            # Use token_to_orig for accurate estimated_start
            min_token_idx = min(token_indices) if token_indices else 0
            if token_to_orig and min_token_idx in token_to_orig:
                estimated_start = max(0, token_to_orig[min_token_idx] - 5)
            else:
                # Fallback: linear interpolation (less accurate for high-disfluency files)
                estimated_start = max(0, int(min_token_idx * len(original_words) / len(processed_doc)) - 5)

            # Find word indices, considering gaps in token_indices
            # Skip already-claimed word indices; prefer fluent over disfluent matches
            word_indices = self._find_word_indices_with_gaps(
                clause_token_texts, token_indices, original_words, 
                min_start=estimated_start, skip_indices=claimed_word_indices,
                disfluency_labels=disfluency_labels
            )

            if word_indices:
                # --- Distance Guard ---
                # If mapping drifted too far from expected position, discard it.
                # This prevents greedy search from claiming words that belong to
                # a later clause (e.g. "an" matching 70 positions ahead).
                actual_start = min(word_indices)
                max_drift = max(20, len(token_indices) * 2)
                if abs(actual_start - estimated_start) > max_drift:
                    continue  # Let this clause become a fragment instead

                # --- Gap Filling for Repetitions/Fillers ---
                # If we skipped words that are repetitions or fillers, claim them now
                # to prevent fragmentation.
                filled_indices = sorted(word_indices)
                if len(filled_indices) > 1:
                    i = 0
                    while i < len(filled_indices) - 1:
                        curr_idx = filled_indices[i]
                        next_idx = filled_indices[i+1]
                        
                        # Check gaps
                        if next_idx - curr_idx > 1:
                            gap_range = range(curr_idx + 1, next_idx)
                            # Check if all words in gap are safe to include (fillers or repetitions)
                            # A repetition match usually matches the *preceding* word.
                            
                            candidates = []
                            for gap_idx in gap_range:
                                gap_word = tg_words[gap_idx].text.lower()
                                prev_word = tg_words[gap_idx - 1].text.lower() # Naive check against immediate predecessor
                                
                                # Check against the last *claimed* word in this clause so far? 
                                # actually gap_idx-1 is usually curr_idx if it's the first in gap.
                                
                                is_filler = gap_word in self.segmenter.fillers
                                is_repetition = (gap_word == prev_word) or (gap_word == tg_words[curr_idx].text.lower())
                                
                                if is_filler or is_repetition:
                                    candidates.append(gap_idx)
                                else:
                                    # If we hit a non-repetition/non-filler (e.g. subordinate clause word), 
                                    # stop filling this gap entirely? or just stop here?
                                    # Usually safe to stop filling.
                                    pass
                                    
                            # If we found candidates, add them
                            if candidates:
                                filled_indices.extend(candidates)
                                
                        i += 1
                        filled_indices.sort()
                        # Restart loop or continue? Sorted so okay.
                
                word_indices = sorted(list(set(filled_indices)))
                # -------------------------------------------

                # VALIDATION: Check if reconstructed text matches expected clause text
                # This prevents misalignment where wrong words are matched
                reconstructed_text = " ".join(tg_words[i].text for i in word_indices if tg_words[i].text)
                
                # Use spaCy POS to identify content words (NOUN, VERB, ADJ, ADV)
                content_pos = {'NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN'}
                
                clause_doc = self.segmenter.nlp(clause_text)
                recon_doc = self.segmenter.nlp(reconstructed_text)
                
                clause_content = {t.lemma_.lower() for t in clause_doc if t.pos_ in content_pos}
                recon_content = {t.lemma_.lower() for t in recon_doc if t.pos_ in content_pos}
                
                # If clause has content words, at least half should appear in reconstructed text
                if clause_content:
                    overlap = len(clause_content & recon_content)
                    overlap_ratio = overlap / len(clause_content)
                    
                    if overlap_ratio < 0.5:
                        # Misalignment detected - skip this clause
                        continue

                # Find contiguous groups, but allow small gaps with repetitions/fillers/disfluencies
                # This handles cases like "they look [they look] happy" where middle is repetition
                # and "put it [in to] on his head" where gap is disfluent false start
                contiguous_groups = []
                current_group = [word_indices[0]]
                for i in range(1, len(word_indices)):
                    gap_size = word_indices[i] - word_indices[i-1] - 1
                    if gap_size == 0:
                        # Contiguous
                        current_group.append(word_indices[i])
                    elif gap_size <= 5:
                        # Small gap - check if it's repetitions/fillers/disfluencies
                        gap_range = range(word_indices[i-1]+1, word_indices[i])
                        gap_words = [tg_words[j].text.lower() for j in gap_range]
                        prev_words = [tg_words[j].text.lower() for j in current_group[-2:] if j < len(tg_words)]
                        
                        # Gap is safe if ALL words in it are either:
                        # - fillers/repetitions (original logic)
                        # - disfluent per the model (new v3 logic)
                        is_safe_gap = True
                        for j in gap_range:
                            w = tg_words[j].text.lower()
                            if not w:
                                continue
                            is_filler = w in self.segmenter.fillers
                            is_repetition = w in prev_words
                            is_disfluent = (disfluency_labels is not None and disfluency_labels[j] == 1)
                            if not (is_filler or is_repetition or is_disfluent):
                                is_safe_gap = False
                                break
                        
                        if is_safe_gap:
                            # Include gap words and continue
                            for j in gap_range:
                                current_group.append(j)
                            current_group.append(word_indices[i])
                        else:
                            contiguous_groups.append(current_group)
                            current_group = [word_indices[i]]
                    else:
                        # Large gap - split
                        contiguous_groups.append(current_group)
                        current_group = [word_indices[i]]
                contiguous_groups.append(current_group)
                
                # Use the largest contiguous group
                largest_group = max(contiguous_groups, key=len)
                
                # Recalculate text and timing from largest contiguous group
                # Exclude disfluent words from text (but keep them in timing span)
                if disfluency_labels is not None:
                    final_text = " ".join(
                        tg_words[i].text for i in sorted(largest_group)
                        if tg_words[i].text and disfluency_labels[i] == 0
                    )
                    clause_syllables = sum(
                        tg_words[i].syllable_count for i in largest_group
                        if disfluency_labels[i] == 0
                    )
                else:
                    final_text = " ".join(tg_words[i].text for i in sorted(largest_group) if tg_words[i].text)
                    clause_syllables = sum(tg_words[i].syllable_count for i in largest_group)
                
                # Claim only the contiguous word indices
                claimed_word_indices.update(largest_group)

                aligned_clauses.append({
                    'start_time': tg_words[largest_group[0]].start,
                    'end_time': tg_words[largest_group[-1]].end,
                    'text': final_text,
                    'type': clause_type,
                    'word_indices': largest_group,
                    'syllables': clause_syllables
                })

        # Sort by start time, with subordinate clauses first when times overlap
        main_types = {'independent', 'independent_copula_less', 'coordinated_vp', 'parataxis'}

        def sort_key(c):
            is_main = 1 if c['type'] in main_types else 0
            return (c['start_time'], is_main, c['end_time'])

        aligned_clauses.sort(key=sort_key)

        aligned_clauses.sort(key=sort_key)

        # === CATCH-ALL: Recover Unclaimed Words ===
        # Finds words that were skipped by the segmenter (e.g. fragments, noun phrases, isolated words)
        # and creates 'fragment' clauses for them so they appear in the TextGrid.
        
        # === CATCH-ALL: Recover Unclaimed Words ===
        # Finds words that were skipped by the segmenter (e.g. fragments, noun phrases, isolated words)
        # and creates 'fragment' clauses for them so they appear in the TextGrid.
        
        all_indices = sorted(list(set(range(len(tg_words))) - claimed_word_indices))
        if all_indices:
            # Group contiguous indices
            groups = []
            if all_indices:
                curr_group = [all_indices[0]]
                for i in range(1, len(all_indices)):
                    if all_indices[i] == all_indices[i-1] + 1:
                        curr_group.append(all_indices[i])
                    else:
                        groups.append(curr_group)
                        curr_group = [all_indices[i]]
                groups.append(curr_group)
            
            # Handle unclaimed words - only merge SHORT groups of function words
            # Do NOT merge groups containing potential clause heads (verbs)
            verb_tags = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}
            for group in groups:
                 valid_indices = [idx for idx in group if not tg_words[idx].is_pause and tg_words[idx].text]
                 if not valid_indices:
                     continue

                 group_start = tg_words[valid_indices[0]].start
                 group_end = tg_words[valid_indices[-1]].end
                 group_text = " ".join([tg_words[idx].text for idx in valid_indices])
                 
                 # Check if group contains a verb - if so, don't merge (could be failed clause)
                 group_doc = self.segmenter.nlp(group_text)
                 has_verb = any(t.tag_ in verb_tags or t.pos_ == 'VERB' for t in group_doc)
                 
                 # Only merge if group is small (≤3 words) AND has no verbs
                 if len(valid_indices) <= 3 and not has_verb:
                     # Find the nearest clause to merge into
                     best_clause = None
                     best_distance = float('inf')
                     
                     for clause in aligned_clauses:
                         if group_end <= clause['start_time']:
                             dist = clause['start_time'] - group_end
                         elif group_start >= clause['end_time']:
                             dist = group_start - clause['end_time']
                         else:
                             dist = 0
                         
                         if dist < best_distance:
                             best_distance = dist
                             best_clause = clause
                     
                     if best_clause and best_distance < 1.0:
                         # Merge: extend clause boundaries and add indices
                         best_clause['word_indices'] = sorted(set(best_clause['word_indices'] + valid_indices))
                         best_clause['start_time'] = min(best_clause['start_time'], group_start)
                         best_clause['end_time'] = max(best_clause['end_time'], group_end)
                         if disfluency_labels is not None:
                             best_clause['text'] = " ".join([tg_words[i].text for i in best_clause['word_indices'] if disfluency_labels[i] == 0])
                             best_clause['syllables'] = sum(tg_words[i].syllable_count for i in best_clause['word_indices'] if disfluency_labels[i] == 0)
                         else:
                             best_clause['text'] = " ".join([tg_words[i].text for i in best_clause['word_indices']])
                             best_clause['syllables'] = sum(tg_words[i].syllable_count for i in best_clause['word_indices'])
                         continue
                 
                 # Larger groups (>3 words), groups with verbs, or no nearby clause - create fragment
                 # But first: try to recover valid clauses from verb-containing groups
                 recovered_type = 'fragment'
                 if has_verb and len(valid_indices) >= 1:
                     non_filler = [tg_words[idx].text.lower().strip(".,!?;:-\"'")
                                   for idx in valid_indices
                                   if tg_words[idx].text.lower().strip(".,!?;:-\"'") not in self.segmenter.fillers]
                     if len(non_filler) >= 3:
                         # Group has a verb and enough content words → treat as a failed clause
                         recovered_type = 'parataxis'
                         
                         # Split at last conjunction AFTER the last verb/particle.
                         # This separates the verb clause from trailing NP starters
                         # e.g. "a baby bird pop out | and the little bird"
                         CONJ_SPLIT = {'and', 'but', 'so', 'then', 'or'}
                         last_verb_pos = -1
                         for vi_pos, vi_idx in enumerate(valid_indices):
                             w = tg_words[vi_idx].text.lower()
                             # Track last verb or verb-particle
                             for t in group_doc:
                                 if t.text.lower() == w and (t.tag_ in verb_tags or t.pos_ == 'VERB' or t.dep_ == 'compound:prt'):
                                     last_verb_pos = vi_pos
                                     break
                         # Find the first conjunction AFTER the last verb
                         split_pos = -1
                         if last_verb_pos >= 0 and last_verb_pos < len(valid_indices) - 2:
                             for vi_pos in range(last_verb_pos + 1, len(valid_indices)):
                                 if tg_words[valid_indices[vi_pos]].text.lower() in CONJ_SPLIT:
                                     split_pos = vi_pos
                                     break
                         # Only split if both parts are non-trivial
                         if split_pos > 0 and split_pos < len(valid_indices) - 1:
                             head_indices = valid_indices[:split_pos]
                             tail_indices = valid_indices[split_pos:]
                             # Re-check: head must still have the verb
                             head_text = " ".join([tg_words[idx].text for idx in head_indices])
                             head_doc = self.segmenter.nlp(head_text)
                             head_has_verb = any(t.tag_ in verb_tags or t.pos_ == 'VERB' for t in head_doc)
                             if head_has_verb and len(head_indices) >= 2:
                                 # Create parataxis for head (verb clause)
                                 if disfluency_labels is not None:
                                     h_text = " ".join([tg_words[i].text for i in head_indices if disfluency_labels[i] == 0])
                                     h_syl = sum(tg_words[i].syllable_count for i in head_indices if disfluency_labels[i] == 0)
                                 else:
                                     h_text = " ".join([tg_words[i].text for i in head_indices])
                                     h_syl = sum(tg_words[i].syllable_count for i in head_indices)
                                 aligned_clauses.append({
                                     'start_time': tg_words[head_indices[0]].start,
                                     'end_time': tg_words[head_indices[-1]].end,
                                     'text': h_text, 'type': 'parataxis',
                                     'word_indices': head_indices, 'syllables': h_syl
                                 })
                                 # Tail becomes a separate fragment (will be merged by Pass 3)
                                 if disfluency_labels is not None:
                                     t_text = " ".join([tg_words[i].text for i in tail_indices if disfluency_labels[i] == 0])
                                     t_syl = sum(tg_words[i].syllable_count for i in tail_indices if disfluency_labels[i] == 0)
                                 else:
                                     t_text = " ".join([tg_words[i].text for i in tail_indices])
                                     t_syl = sum(tg_words[i].syllable_count for i in tail_indices)
                                 aligned_clauses.append({
                                     'start_time': tg_words[tail_indices[0]].start,
                                     'end_time': tg_words[tail_indices[-1]].end,
                                     'text': t_text, 'type': 'fragment',
                                     'word_indices': tail_indices, 'syllables': t_syl
                                 })
                                 claimed_word_indices.update(valid_indices)
                                 continue

                 if len(valid_indices) >= 1:
                     if disfluency_labels is not None:
                         frag_text = " ".join([tg_words[idx].text for idx in valid_indices if disfluency_labels[idx] == 0])
                         frag_syl = sum(tg_words[idx].syllable_count for idx in valid_indices if disfluency_labels[idx] == 0)
                     else:
                         frag_text = " ".join([tg_words[idx].text for idx in valid_indices])
                         frag_syl = sum(tg_words[idx].syllable_count for idx in valid_indices)
                     aligned_clauses.append({
                        'start_time': group_start,
                        'end_time': group_end,
                        'text': frag_text,
                        'type': recovered_type,
                        'word_indices': valid_indices,
                        'syllables': frag_syl
                     })



        aligned_clauses.sort(key=sort_key)



        return aligned_clauses



    def _find_word_indices_with_gaps(self, clause_tokens: List[str],
                                      token_indices: List[int],
                                      original_words: List[str],
                                      min_start: int = 0,
                                      skip_indices: set = None,
                                      disfluency_labels: List[int] = None) -> List[int]:
        """Find matching word indices, respecting gaps in token indices.
        
        When token_indices have gaps (non-contiguous), we split into groups
        and match each group separately, ensuring gaps between groups are reflected.
        
        skip_indices: set of word indices already claimed by other clauses - these are skipped.
        """
        if not clause_tokens:
            return []
        
        if skip_indices is None:
            skip_indices = set()

        clause_clean = [t.strip('.,!?;:-\"\\\'').lower() for t in clause_tokens]
        orig_clean = [w.strip('.,!?;:-\"\\\'').lower() for w in original_words]

        def tokens_match(clause_tok, orig_tok, exact_only=False):
            """Check if tokens match, handling contractions
            exact_only: if True, only return True for exact matches
            """
            # Allow empty clause tokens (stripped punctuation) to match anything within the flow
            if not clause_tok:
                 return True
                 
            if clause_tok == orig_tok:
                return True
            
            if exact_only:
                return False

            # Handle contractions (but only if not requiring exact match)
            if "'" in orig_tok:
                base = orig_tok.split("'")[0]
                ending = orig_tok.split("'")[1] if "'" in orig_tok else ""
                if clause_tok == base or clause_tok == ending or clause_tok == "n't" or clause_tok == "nt":
                    return True
            if "n't" in orig_tok:
                base = orig_tok.replace("n't", "")
                if clause_tok == base or clause_tok == "nt" or clause_tok == "n't":
                    return True
                    
            # Fallback patch for tricky "sorry" alignment (hkk edge case)
            if "sorry" in clause_tok and "sorry" in orig_tok:
                return True
                
            return False
        
        def is_exact_match(clause_tok, orig_tok):
            """Check if this is an exact match (not a contraction match)"""
            if not clause_tok:
                return False
            return clause_tok == orig_tok


        # Find groups of contiguous token indices
        groups = []
        if token_indices:
            current_group_tokens = [clause_clean[0]]
            current_group_start_idx = token_indices[0]
            
            for i in range(1, len(token_indices)):
                if token_indices[i] == token_indices[i-1] + 1:
                    # Contiguous with previous
                    current_group_tokens.append(clause_clean[i])
                else:
                    # Gap found - save current group and start new one
                    gap_size = token_indices[i] - token_indices[i-1] - 1
                    groups.append({
                        'tokens': current_group_tokens,
                        'start_idx': current_group_start_idx,
                        'gap_after': gap_size
                    })
                    current_group_tokens = [clause_clean[i]]
                    current_group_start_idx = token_indices[i]
            
            # Add final group
            groups.append({
                'tokens': current_group_tokens,
                'start_idx': current_group_start_idx,
                'gap_after': 0
            })

        # Match each group, requiring gaps between groups
        all_indices = []
        search_start = min_start

        for group_idx, group in enumerate(groups):
            group_tokens = group['tokens']
            gap_after = group['gap_after']
            
            # Find best match for this group starting from search_start
            # Prefer exact matches over contraction matches
            best_start = search_start
            best_score = 0
            best_exact_count = 0
            
            for start in range(search_start, len(orig_clean)):
                # Skip if this start position is claimed
                if start in skip_indices:
                    continue
                    
                score = 0
                exact_count = 0
                j = 0
                for i in range(start, min(start + len(group_tokens) * 2, len(orig_clean))):
                    if i in skip_indices:  # Skip claimed indices
                        continue
                    if j < len(group_tokens):
                        if tokens_match(group_tokens[j], orig_clean[i]):
                            score += 1
                            if is_exact_match(group_tokens[j], orig_clean[i]):
                                exact_count += 1
                            j += 1
                            # Handle contractions in scoring too
                            while j < len(group_tokens) and tokens_match(group_tokens[j], orig_clean[i]):
                                score += 0.5 # Give generic bonus for contraction parts
                                j += 1
                # Prefer matches with more exact matches
                if score > best_score or (score == best_score and exact_count > best_exact_count):
                    best_score = score
                    best_exact_count = exact_count
                    best_start = start

            # Collect indices for this group
            # When multiple words could match, prefer exact matches
            j = 0
            for i in range(best_start, len(orig_clean)):
                if j >= len(group_tokens):
                    break
                if i in skip_indices:  # Skip claimed indices
                    continue
                
                # Check if there's an exact match ahead - if so, skip contraction matches
                if tokens_match(group_tokens[j], orig_clean[i]):
                    # Check if this is a contraction match and there's an exact match soon
                    if not is_exact_match(group_tokens[j], orig_clean[i]):
                        # Look ahead for exact match within next 3 words
                        found_exact_ahead = False
                        for lookahead in range(i + 1, min(i + 4, len(orig_clean))):
                            if lookahead not in skip_indices and is_exact_match(group_tokens[j], orig_clean[lookahead]):
                                found_exact_ahead = True
                                break
                        if found_exact_ahead:
                            continue  # Skip this contraction match, prefer exact ahead
                    
                    # Prefer non-disfluent match: if this word is disfluent and
                    # there's a fluent match ahead, skip this one
                    if disfluency_labels is not None and disfluency_labels[i] == 1:
                        found_fluent_ahead = False
                        for lookahead in range(i + 1, min(i + 8, len(orig_clean))):
                            if lookahead not in skip_indices and tokens_match(group_tokens[j], orig_clean[lookahead]):
                                if disfluency_labels[lookahead] == 0:
                                    found_fluent_ahead = True
                                    break
                        if found_fluent_ahead:
                            continue  # Skip disfluent match, prefer fluent ahead
                    
                    all_indices.append(i)
                    j += 1
                    # Handle contractions consuming multiple tokens
                    while j < len(group_tokens) and tokens_match(group_tokens[j], orig_clean[i]):
                        j += 1

            # Set search_start for next group - require a gap
            if all_indices and gap_after > 0:
                # Skip at least gap_after words (proportionally adjusted)
                search_start = all_indices[-1] + max(1, gap_after)
            elif all_indices:
                search_start = all_indices[-1] + 1

        return all_indices




# ==============================================================================
# Processing Functions
# ==============================================================================

def process_single_textgrid(input_path: str, output_path: str,
                           segmenter: ClauseSegmenter) -> List[str]:
    """Process a single TextGrid file. Returns clause_log_lines."""
    filename = os.path.basename(input_path)
    print(f"\nProcessing: {filename}")
    
    # Filler words to check
    FILLERS = {'uh', 'um', 'er', 'ah', 'uhm', 'mm', 'hmm', 'eh', 'uh-huh', 'mm-hmm'}

    tg = TextGridHandler(input_path)
    tg_words = tg.get_word_list()
    print(f"  Words: {len(tg_words)}, Syllables: {tg.total_syllables}")

    transcript = tg.get_transcript()
    logical_clauses = segmenter.segment(
        transcript,
        verbose=False
    )

    aligner = ClauseAligner(tg, segmenter)
    clauses = aligner.align_clauses()
    
    # Filter out clauses that only contain fillers or have too few words.
    # Per Vercellotti: clause = verb + complement/adjunct → minimum 2 words.
    # Single-word aligned clauses are alignment artifacts (drift bug).
    filtered_clauses = []
    for c in clauses:
        if 'word_indices' in c and c['word_indices']:
            words = [tg_words[i].text.lower().strip(".,!?;:-\"'") for i in c['word_indices']]
            non_filler_words = [w for w in words if w and w not in FILLERS]
            if len(non_filler_words) == 0:
                pass  # Skip filler-only clauses
            elif len(non_filler_words) <= 1 and c['type'] != 'fragment':
                c['type'] = 'fragment'
                filtered_clauses.append(c)
            else:
                filtered_clauses.append(c)
        else:
            filtered_clauses.append(c)
    
    non_fragment_clauses = [c for c in filtered_clauses if c['type'] != 'fragment']
    fragment_clauses = [c for c in filtered_clauses if c['type'] == 'fragment']

    print(f"  Clauses (Logical): {len(logical_clauses)}")
    print(
        f"  Clauses (Aligned, non-fragment): {len(non_fragment_clauses)} "
        f"(filtered from {len(clauses)}; fragments: {len(fragment_clauses)})"
    )
    
    # Add clauses to TextGrid and get final split intervals

    final_intervals = tg.add_clause_tier(filtered_clauses, tier_name="clauses")
    # Save to output path
    tg.save(output_path)
    print(f"  Saved: {output_path}")

    # Build clause log lines from actual split intervals
    clause_log = []
    clause_log.append(f"File: {filename}")
    clause_log.append(f"Words: {len(tg_words)}, Syllables: {tg.total_syllables}")
    clause_log.append(f"Clauses (Logical): {len(logical_clauses)}")
    clause_log.append(f"Clauses (Aligned, non-fragment): {len(non_fragment_clauses)}")
    clause_log.append(f"Fragments (Not Counted): {len(fragment_clauses)}")
    clause_log.append(f"Intervals (Split): {len(final_intervals)}")
    clause_log.append("-" * 80)

    # Show logical clauses (segmenter output)
    clause_log.append("Logical clauses (segmenter output)")
    for _, _, ctype, ctext, _ in logical_clauses:
        clean_text = ' '.join(ctext.split())
        clause_log.append(f"{ctype:<25} | {clean_text}")
    clause_log.append("")

    # Show actual split intervals (TextGrid tier) - PRINT TO CONSOLE
    clause_log.append("Split intervals (TextGrid tier)")
    for c in final_intervals:
        actual_text = c['label']
        display_text = actual_text[:60] + '...' if len(actual_text) > 60 else actual_text
        print(f"    {c['type']:<25} | {display_text}")
        clause_log.append(f"{c['type']:<25} | {actual_text}")
    
    clause_log.append("")  # Blank line between files

    return clause_log


# ==============================================================================
# Main
# ==============================================================================



# ==============================================================================
# V3 ClauseSegmenter - Sentence Support + Neural Disfluency Detection
# ==============================================================================

class ClauseSegmenterV3(ClauseSegmenter):
    """
    Extends ClauseSegmenter with:
    - wtpsplit sentence support (from v2)
    - Neural disfluency detection (v3 addition)
    Uses EXACTLY the same clause detection rules as v1/v2.
    """
    
    def __init__(self, nlp, preprocess: bool = True, debug: bool = False,
                 sentences_dir: str = None, disfluency_detector: DisfluencyDetector = None):
        super().__init__(nlp, preprocess=preprocess, debug=debug,
                        disfluency_detector=disfluency_detector)
        self.sentence_loader = SentenceLoader(sentences_dir)
        self._current_filename = None
    
    def segment(self, text: str, verbose: bool = True,
                remove_fillers: bool = False,
                handle_repetitions: bool = True):
        """Segment using EXACT same logic as original."""
        
        if self.preprocess:
            text, info = self.preprocess_text(text, remove_fillers, handle_repetitions)
            disfluencies = info.get('disfluencies_removed', 0)
            fillers = info.get('fillers_removed', 0)
            if verbose and (disfluencies > 0 or fillers > 0):
                print(f"Preprocessing (disfluency model):")
                print(f"  - Disfluencies removed: {disfluencies}")
                if info.get('removed_words'):
                    preview = info['removed_words'][:10]
                    print(f"  - Removed: {', '.join(preview)}{'...' if len(info['removed_words']) > 10 else ''}")
                print(f"  - Words: {info['original_length']} -> {info['processed_length']}")
                print()
        
        # Parse full text as single doc - SAME AS ORIGINAL
        doc = self.nlp(text)
        
        # Use original's find_all_clauses - SAME AS ORIGINAL
        clauses = self.find_all_clauses(doc)
        
        if verbose:
            print(f"{'TYPE':<30} | {'CLAUSE TEXT'}")
            print("-" * 110)
            for start, end, ctype, ctext, indices in clauses:
                clean = ' '.join(ctext.split())
                if len(clean) > 75:
                    clean = clean[:72] + "..."
                print(f"{ctype:<30} | {clean}")
            print(f"\nTotal clauses: {len(clauses)}")
        
        return clauses


# ==============================================================================
# V3 Processing Functions
# ==============================================================================

def split_clauses_at_sentence_boundaries(clauses: List[dict], tg_words: List, 
                                          sentences: List[str]) -> List[dict]:
    """
    Post-process clauses to split any that span multiple sentences.
    This enforces sentence boundaries as clause boundaries.
    """
    if not sentences:
        return clauses
    
    # Build word-to-sentence mapping
    word_to_sent = {}
    word_idx = 0
    for sent_idx, sent in enumerate(sentences):
        sent_words = sent.lower().split()
        for sw in sent_words:
            if word_idx < len(tg_words):
                word_to_sent[word_idx] = sent_idx
                word_idx += 1
    
    # Fill remaining words
    for i in range(word_idx, len(tg_words)):
        word_to_sent[i] = len(sentences) - 1
    
    # Split clauses that span multiple sentences
    new_clauses = []
    for c in clauses:
        if 'word_indices' not in c or not c['word_indices']:
            new_clauses.append(c)
            continue
        
        indices = c['word_indices']
        if not indices:
            new_clauses.append(c)
            continue
        
        # Group word indices by sentence
        sent_groups = {}
        for idx in indices:
            sent_idx = word_to_sent.get(idx, 0)
            if sent_idx not in sent_groups:
                sent_groups[sent_idx] = []
            sent_groups[sent_idx].append(idx)
        
        # If clause spans only one sentence, keep as-is
        if len(sent_groups) == 1:
            new_clauses.append(c)
        else:
            # Split into multiple clauses
            for sent_idx in sorted(sent_groups.keys()):
                group_indices = sent_groups[sent_idx]
                words = [tg_words[i] for i in group_indices]
                text = ' '.join(w.text for w in words)
                new_clause = {
                    'type': c['type'],
                    'word_indices': group_indices,
                    'start_time': words[0].start,
                    'end_time': words[-1].end,
                    'text': text,
                    'label': text
                }
                new_clauses.append(new_clause)
    
    # Sort by start time
    new_clauses.sort(key=lambda x: x.get('start_time', 0))
    return new_clauses


def process_single_textgrid_v3(input_path: str, output_path: str,
                                segmenter: ClauseSegmenter) -> List:
    """Process a single TextGrid file with V3 segmenter (neural disfluency + sentences)."""
    filename = os.path.basename(input_path)
    print(f"\nProcessing: {filename}")
    
    # Set filename for sentence loading
    segmenter._current_filename = filename
    
    # Load sentences for post-processing
    sentences = segmenter.sentence_loader.load_sentences(filename)
    
    # Use original's TextGridHandler
    tg = TextGridHandler(input_path)
    tg_words = tg.get_word_list()
    print(f"  Words: {len(tg_words)}, Syllables: {tg.total_syllables}")

    transcript = tg.get_transcript()
    logical_clauses = segmenter.segment(transcript, verbose=False)

    # Get disfluency labels for word-level filtering
    disfluency_labels = None
    if segmenter.disfluency_detector is not None:
        words_text = [w.text for w in tg_words]
        disfluency_labels = segmenter.disfluency_detector.detect(words_text)
        # Force-mark <unk>/unk as disfluent
        UNK_TOKENS = {'<unk>', 'unk'}
        for i, w in enumerate(tg_words):
            if w.text.lower() in UNK_TOKENS:
                disfluency_labels[i] = 1

    # Use original's ClauseAligner 
    aligner = ClauseAligner(tg, segmenter)
    clauses = aligner.align_clauses(disfluency_labels=disfluency_labels)
    
    # Filter out clauses that only contain fillers or have too few words.
    # Per Vercellotti: clause = verb + complement/adjunct → minimum 2 words.
    # Single-word aligned clauses are alignment artifacts (drift bug).
    # Also check displayed text (after disfluency removal) — a clause with many
    # word_indices but only 1 displayed word should be treated as a fragment.
    FILLERS = {'uh', 'um', 'er', 'ah', 'uhm', 'mm', 'hmm', 'eh', '<unk>', 'unk'}
    filtered_clauses = []
    for c in clauses:
        if 'word_indices' in c and c['word_indices']:
            words = [tg_words[i].text.lower().strip(".,!?;:-\"'") for i in c['word_indices']]
            non_filler_words = [w for w in words if w and w not in FILLERS]
            # Also check displayed text word count
            displayed_words = [w for w in c.get('text', '').split() if w.strip()]
            displayed_non_filler = [w for w in displayed_words if w.lower().strip(".,!?;:-\"'") not in FILLERS]
            effective_count = min(len(non_filler_words), len(displayed_non_filler))
            if effective_count == 0:
                pass  # Skip filler-only clauses
            elif effective_count <= 1 and c['type'] != 'fragment':
                c['type'] = 'fragment'
                filtered_clauses.append(c)
            else:
                filtered_clauses.append(c)
        else:
            filtered_clauses.append(c)
    
    # === Pass 2: Merge verbless non-fragment clauses into nearest neighbor ===
    # Per Vercellotti: every clause needs a verb. Verbless clauses are alignment
    # artifacts where the verb ended up in a different interval.
    # Conservative function-word set: words that can NEVER be a verb.
    NEVER_VERB = {
        # Pronouns
        'i', 'me', 'my', 'mine', 'myself',
        'you', 'your', 'yours', 'yourself',
        'he', 'him', 'his', 'himself',
        'she', 'her', 'hers', 'herself',
        'it', 'its', 'itself',
        'we', 'us', 'our', 'ours', 'ourselves',
        'they', 'them', 'their', 'theirs', 'themselves',
        # Determiners
        'a', 'an', 'the', 'this', 'that', 'these', 'those',
        'some', 'any', 'no', 'every', 'each', 'all', 'both',
        'another', 'other', 'such',
        # Conjunctions
        'and', 'but', 'or', 'so', 'yet', 'nor',
        # Prepositions / subordinators
        'of', 'in', 'on', 'at', 'to', 'for', 'with', 'by',
        'from', 'as', 'into', 'about', 'after', 'before',
        'while', 'until', 'since', 'because', 'if', 'than',
        'between', 'through', 'during', 'without', 'against',
        # Adverbs
        'very', 'really', 'just', 'also', 'too', 'suddenly',
        'soon', 'then', 'now', 'here', 'there', 'not',
        'only', 'already', 'still', 'again', 'never', 'always',
        'maybe', 'finally', 'actually', 'next', 'apparently',
        # WH-words
        'what', 'who', 'whom', 'which', 'where', 'when', 'how', 'why',
        # Interjections / discourse
        'oh', 'wow', 'okay', 'ok', 'well', 'yes', 'yeah', 'hm',
        # Common nouns/adjectives that never function as verbs in L2 narratives
        'bird', 'boy', 'girl', 'man', 'woman', 'kid', 'baby',
        'hat', 'story', 'thing', 'day', 'time', 'snake', 'elephant',
        'cat', 'mouse', 'dragon', 'tree', 'animal', 'animals',
        'snail', 'tiger', 'floor', 'ground', 'park', 'home',
        'bubble', 'bubbles', 'feather', 'nest', 'egg',
        'little', 'big', 'small', 'black', 'white', 'brown',
        'red', 'blue', 'green', 'new', 'old', 'young',
        # 'like' as preposition/conjunction in fragments (e.g., "like animals")
        'like',
    }
    merged_clauses = []
    verbless_to_merge = []
    for c in filtered_clauses:
        if c['type'] == 'fragment' or 'copula_less' in c['type']:
            merged_clauses.append(c)
            continue
        if 'word_indices' in c and c['word_indices']:
            # Use displayed text (fluent words only) for verbless check.
            # Raw word_indices may include disfluent verbs that inflate the word list.
            displayed = [w.lower().strip(".,!?;:-\"'") for w in c.get('text', '').split() if w.strip()]
            non_filler_words = [w for w in displayed if w and w not in FILLERS]
            if non_filler_words and all(w in NEVER_VERB for w in non_filler_words):
                verbless_to_merge.append(c)
            else:
                merged_clauses.append(c)
        else:
            merged_clauses.append(c)

    for vc in verbless_to_merge:
        best_clause = None
        best_distance = float('inf')
        for clause in merged_clauses:
            if clause['type'] == 'fragment' or 'word_indices' not in clause:
                continue
            if vc['end_time'] <= clause['start_time']:
                dist = clause['start_time'] - vc['end_time']
            elif vc['start_time'] >= clause['end_time']:
                dist = vc['start_time'] - clause['end_time']
            else:
                dist = 0
            if dist < best_distance:
                best_distance = dist
                best_clause = clause
        if best_clause and best_distance < 2.0:
            best_clause['word_indices'] = sorted(set(best_clause['word_indices'] + vc['word_indices']))
            best_clause['start_time'] = min(best_clause['start_time'], vc['start_time'])
            best_clause['end_time'] = max(best_clause['end_time'], vc['end_time'])
            if disfluency_labels is not None:
                best_clause['text'] = " ".join(
                    tg_words[i].text for i in best_clause['word_indices']
                    if disfluency_labels[i] == 0
                )
                best_clause['syllables'] = sum(
                    tg_words[i].syllable_count for i in best_clause['word_indices']
                    if disfluency_labels[i] == 0
                )
            else:
                best_clause['text'] = " ".join([tg_words[i].text for i in best_clause['word_indices']])
                best_clause['syllables'] = sum(tg_words[i].syllable_count for i in best_clause['word_indices'])
        else:
            vc['type'] = 'fragment'
            merged_clauses.append(vc)
    if verbless_to_merge:
        print(f"  Verbless merged: {len(verbless_to_merge)}")
    filtered_clauses = merged_clauses

    # === Pass 3: Merge remaining fragments into nearest non-fragment neighbor ===
    # Fragments (single-word, verbless) should not appear as standalone clauses.
    # Merge them into the nearest real clause by extending its time span.
    non_fragment = [c for c in filtered_clauses if c['type'] != 'fragment']
    fragments = [c for c in filtered_clauses if c['type'] == 'fragment']
    frag_merged = 0
    for frag in fragments:
        # Large fragments (>5 displayed words) should not be merged — reclassify as parataxis
        displayed = [w for i in frag.get('word_indices', []) if (disfluency_labels is None or disfluency_labels[i] == 0) for w in [tg_words[i].text] if w.strip()]
        if len(displayed) > 5:
            frag['type'] = 'parataxis'
            non_fragment.append(frag)
            continue
        best_clause = None
        best_distance = float('inf')
        for clause in non_fragment:
            if 'word_indices' not in clause:
                continue
            if frag['end_time'] <= clause['start_time']:
                dist = clause['start_time'] - frag['end_time']
            elif frag['start_time'] >= clause['end_time']:
                dist = frag['start_time'] - clause['end_time']
            else:
                dist = 0
            if dist < best_distance:
                best_distance = dist
                best_clause = clause
        if best_clause and best_distance < 5.0:
            best_clause['word_indices'] = sorted(set(best_clause.get('word_indices', []) + frag.get('word_indices', [])))
            best_clause['start_time'] = min(best_clause['start_time'], frag['start_time'])
            best_clause['end_time'] = max(best_clause['end_time'], frag['end_time'])
            # Reconstruct text respecting disfluency labels
            if disfluency_labels is not None:
                best_clause['text'] = " ".join(
                    tg_words[i].text for i in best_clause['word_indices']
                    if disfluency_labels[i] == 0
                )
            else:
                best_clause['text'] = " ".join(tg_words[i].text for i in best_clause['word_indices'])
            frag_merged += 1
        # else: drop the fragment entirely (too far from any clause)
    if frag_merged:
        print(f"  Fragments merged: {frag_merged}")
    filtered_clauses = non_fragment

    fragments = [c for c in filtered_clauses if c['type'] == 'fragment']

    # === Pass 4: Split over-long clauses at parataxis boundaries ===
    # Clauses >20 displayed words may contain multiple independent clauses
    # that the parser merged. Re-parse and split at parataxis dependencies.
    VERB_TAGS_P4 = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD'}
    split_result = []
    long_split = 0
    for c in filtered_clauses:
        displayed_words = c.get('text', '').split()
        if len(displayed_words) <= 20 or 'word_indices' not in c or not c['word_indices']:
            split_result.append(c)
            continue
        # Re-parse displayed text
        doc = segmenter.nlp(c['text'])
        root = None
        for t in doc:
            if t.dep_ == 'ROOT':
                root = t
                break
        if root is None:
            split_result.append(c)
            continue
        # Find parataxis children of ROOT (independent clauses attached to main)
        para_heads = [t for t in doc if t.dep_ == 'parataxis' and t.head == root]
        if not para_heads:
            split_result.append(c)
            continue
        # Use the first (leftmost) parataxis subtree to find split point
        para_heads.sort(key=lambda t: t.i)
        first_para = para_heads[0]
        leftmost_tok = min(first_para.subtree, key=lambda t: t.i)
        # Map token char offset to displayed word index
        char_pos = leftmost_tok.idx
        running = 0
        split_word_idx = len(displayed_words)  # default: no split
        for wi, dw in enumerate(displayed_words):
            if running + len(dw) > char_pos:
                split_word_idx = wi
                break
            running += len(dw) + 1
        # Both halves must be non-trivial (>= 3 displayed words)
        if split_word_idx < 3 or (len(displayed_words) - split_word_idx) < 3:
            split_result.append(c)
            continue
        # Map split point to original word indices
        fluent_indices = [i for i in c['word_indices']
                          if disfluency_labels is None or disfluency_labels[i] == 0]
        if split_word_idx >= len(fluent_indices):
            split_result.append(c)
            continue
        boundary = fluent_indices[split_word_idx]
        head_indices = [i for i in c['word_indices'] if i < boundary]
        tail_indices = [i for i in c['word_indices'] if i >= boundary]
        if not head_indices or not tail_indices:
            split_result.append(c)
            continue
        # Build head clause
        if disfluency_labels is not None:
            h_text = " ".join(tg_words[i].text for i in head_indices if disfluency_labels[i] == 0)
            h_syl = sum(tg_words[i].syllable_count for i in head_indices if disfluency_labels[i] == 0)
        else:
            h_text = " ".join(tg_words[i].text for i in head_indices)
            h_syl = sum(tg_words[i].syllable_count for i in head_indices)
        split_result.append({
            'start_time': tg_words[head_indices[0]].start,
            'end_time': tg_words[head_indices[-1]].end,
            'text': h_text, 'type': c['type'],
            'word_indices': head_indices, 'syllables': h_syl
        })
        # Build tail clause
        if disfluency_labels is not None:
            t_text = " ".join(tg_words[i].text for i in tail_indices if disfluency_labels[i] == 0)
            t_syl = sum(tg_words[i].syllable_count for i in tail_indices if disfluency_labels[i] == 0)
        else:
            t_text = " ".join(tg_words[i].text for i in tail_indices)
            t_syl = sum(tg_words[i].syllable_count for i in tail_indices)
        split_result.append({
            'start_time': tg_words[tail_indices[0]].start,
            'end_time': tg_words[tail_indices[-1]].end,
            'text': t_text, 'type': 'parataxis',
            'word_indices': tail_indices, 'syllables': t_syl
        })
        long_split += 1
    if long_split:
        print(f"  Long clauses split: {long_split}")
    filtered_clauses = split_result

    print(f"  Clauses (Logical): {len(logical_clauses)}")
    print(f"  Clauses (Aligned): {len([c for c in filtered_clauses if c['type'] != 'fragment'])} (fragments: {len([c for c in filtered_clauses if c['type'] == 'fragment'])})")
    
    # Add disfluency tier (reuse labels computed earlier)
    if disfluency_labels is not None:
        tg.add_disfluency_tier(disfluency_labels, tier_name="disfluency")
    
    # Add clause tier and save
    final_intervals = tg.add_clause_tier(filtered_clauses, tier_name="clauses")
    tg.save(output_path)
    print(f"  Saved: {output_path}")

    # Build log
    clause_log = [f"File: {filename}", f"Clauses: {len(non_fragment)}"]
    for c in (final_intervals or []):
        clause_log.append(f"  {c['type']:<25} | {c['label'][:60]}")
    
    return clause_log


def process_all_textgrids_v3(input_dir: str, output_dir: str,
                              segmenter: ClauseSegmenter):
    """Process all TextGrid files in directory with V3 segmenter."""
    os.makedirs(output_dir, exist_ok=True)
    
    files = sorted([f for f in os.listdir(input_dir) if f.endswith('.TextGrid')])
    print(f"Processing {len(files)} TextGrid files...")
    
    all_logs = []
    for i, filename in enumerate(files, 1):
        input_path = os.path.join(input_dir, filename)
        output_path = os.path.join(output_dir, filename)
        
        try:
            log = process_single_textgrid_v3(input_path, output_path, segmenter)
            all_logs.extend(log)
        except Exception as e:
            print(f"  ERROR: {e}")
            import traceback
            traceback.print_exc()
    
    # Write log
    log_path = os.path.join(output_dir, 'clause_log.txt')
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(all_logs))
    print(f"\nClause log saved to: {log_path}")


# ==============================================================================
# Main
# ==============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="V3 Clause Segmentation with neural disfluency detection",
        epilog="Uses a trained xlm-roberta-base model to remove disfluencies before clause segmentation."
    )
    parser.add_argument("-i", "--input", required=True,
                        help="Input TextGrid file or directory")
    parser.add_argument("-o", "--output", required=True,
                        help="Output TextGrid file or directory")
    parser.add_argument("--sentences", default=None,
                        help="Directory with sentence JSON files from sentence_segmenter.py (optional)")
    parser.add_argument("--spacy-model", default=None,
                        help="Path to spaCy model (default: auto-detect)")
    parser.add_argument("--disfluency-model", default=None,
                        help="Path to disfluency detection model (default: auto-detect in project)")
    parser.add_argument("--no-disfluency", action="store_true",
                        help="Disable neural disfluency detection (fallback to no preprocessing)")
    parser.add_argument("--debug", action="store_true",
                        help="Enable debug output")
    
    args = parser.parse_args()
    
    # Find spaCy model
    model_path = args.spacy_model
    if not model_path:
        candidates = [
            r"d:\Dropbox\zemizemi\article\newcaf\model\en_ud_L1L2e_combined_trf-0.0.1\en_ud_L1L2e_combined_trf\en_ud_L1L2e_combined_trf-0.0.1",
            "/Users/riku/Library/CloudStorage/Dropbox/zemizemi/article/newcaf/model/en_ud_L1L2e_combined_trf-0.0.1/en_ud_L1L2e_combined_trf/en_ud_L1L2e_combined_trf-0.0.1",
            "en_core_web_sm",
        ]
        for c in candidates:
            if os.path.exists(c) or c == "en_core_web_sm":
                model_path = c
                break
    
    print(f"Loading spaCy model: {model_path}")
    nlp = spacy.load(model_path)
    print("spaCy model loaded!")
    
    # Load disfluency model
    disfluency_detector = None
    if not args.no_disfluency:
        try:
            disfluency_detector = DisfluencyDetector(args.disfluency_model)
        except FileNotFoundError as e:
            print(f"WARNING: {e}")
            print("Continuing without disfluency detection.")
    
    # Initialize V3 segmenter
    segmenter = ClauseSegmenterV3(nlp, preprocess=True, debug=args.debug,
                                   sentences_dir=args.sentences,
                                   disfluency_detector=disfluency_detector)
    
    # Process
    if os.path.isfile(args.input):
        os.makedirs(os.path.dirname(args.output) or ".", exist_ok=True)
        process_single_textgrid_v3(args.input, args.output, segmenter)
    else:
        process_all_textgrids_v3(args.input, args.output, segmenter)
    
    print("\nDone! Use caf_calculator.py to calculate CAF measures.")


if __name__ == "__main__":
    main()
